experiment:
  seed: 12
  device: cuda 
  expt_name: dino_vit_base # experiment name
  log_freq: 20 # knn accuracy is computed every `log_freq` epochs, additionally the trained model is also saved
  output_dir: ../working/1407/dino_vit_base # output results to this folder - files: logs.txt, {experiment_name}_models
  ssl_training: dino # training strategy to use
  use_mixed_precision: true # if true uses fp16 for training, improves training speed
# input image parameters
input:
  channels: 3 # num channels in input image
  data path: ../input/real_lenses_dataset # path/to/dataset
  indices: ../input/indices.pkl 
  image size: 
  - 64 # image height to perform center crop
  - 64 # image width to perform center crop
  num classes: 2
# training network parameters
network:
  backbone: vit_base # backbone network
  head_bottleneck_dim: 192 # here vit has embed dim 1024
  head_hidden_dim: 256 # 512
  head_nlayers: 3
  head_norm_last_layer: true
  head_output_dim: 512 # 2^12 
  head_use_bn: true
  patch_size: 16
  use_dense_prediction: false
  window_size: None
  ape: None
  use_L1: false
optimizer:
# small lr, cosine schedule lr, low init wd are good
# expt 5 converged quickly, so trying out a larger lr to see if it will learn more
  init_lr: 0.00005 # anything more than this is unstable
  init_wd: 0.001 # low momentum helps, zero doesn't give better results
  final_lr: 0.00000005 # keeping this low helps, too low makes learning slow
  final_wd: 0.001 # high momentum at the end regularizes
  momentum_teacher: 0.9996 # less value would give higher acc initially then diverge
  optimizer: AdamW
  scheduler_warmup_epochs: 10 
  teacher_temp: 0.04
  warmup_teacher_temp: 0.04
  warmup_teacher_temp_epochs: 0
  clip_grad_magnitude: 0. # check
# restart training from checkpoint file ckpt
restore:
  ckpt_path: null # path/to/ckpt
  restore: false # if true, restart from provided ckpt
# keyword args for the ssl per data augmentation
ssl augmentation kwargs: 
  augmentation: AugmentationDINOSingleChannel
  center_crop: 64
  global_crop_scale_range: 
  - 0.14
  - 1.0
  global_crop_size: 64
  local_crop_scale_range: 
  - 0.01
  - 0.4
  local_crop_size: 28
  num_local_crops: 8
# training parameters
train args:
  knn_neighbours: 20
  batch_size: 256
  num_epochs: 100
  freeze_last_layer: 0
  train_val_split: 
  - 0.85
  - 0.15