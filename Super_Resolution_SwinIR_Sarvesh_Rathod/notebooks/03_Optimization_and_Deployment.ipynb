{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03. Model Optimization and Deployment for Gravitational Lensing\n",
        "\n",
        "**Objective**: Optimize the trained SwinIR model for efficient deployment. \n",
        "\n",
        "In scientific applications (e.g., satellite-based processing), computational resources are often limited. This notebook demonstrates:\n",
        "1. **Weight Pruning**: Removing redundant connections to reduce model complexity.\n",
        "2. **Quantization**: Converting model weights from FP32 to INT8 to reduce size and increase speed.\n",
        "3. **ONNX Export**: Exporting the model for platform-independent, high-performance inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Setup and Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization Pipeline Initialized.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Add parent directory to path to import local modules\n",
        "sys.path.append(os.path.abspath('..'))\n",
        "\n",
        "from model import SwinIR\n",
        "\n",
        "DEVICE = 'cpu' # Optimization is often verified on CPU for deployment targets\n",
        "\n",
        "# Try to find the most recent checkpoint\n",
        "MODEL_PATHS = [\n",
        "    '../swinir_advanced_epoch_final.pth',\n",
        "    '../swinir_advanced_epoch_20.pth',\n",
        "    '../swinir_advanced_epoch_15.pth',\n",
        "    '../swinir_advanced_epoch_10.pth',\n",
        "    '../swinir_advanced_epoch_5.pth',\n",
        "]\n",
        "\n",
        "MODEL_PATH = None\n",
        "for path in MODEL_PATHS:\n",
        "    if os.path.exists(path):\n",
        "        MODEL_PATH = path\n",
        "        break\n",
        "\n",
        "if MODEL_PATH is None:\n",
        "    print(\"⚠️ WARNING: No model checkpoint found. Please train the model first using notebook 02.\")\n",
        "    print(\"Using default model architecture for demonstration.\")\n",
        "    MODEL_PATH = None\n",
        "\n",
        "print(f\"Optimization Pipeline Initialized.\")\n",
        "if MODEL_PATH:\n",
        "    print(f\"Will load model from: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Model Pruning (L1 Unstructured)\n",
        "We remove the smallest 20% of weights in the convolutional layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model first\n",
        "if MODEL_PATH is None:\n",
        "    print(\"⚠️ No checkpoint found. Using untrained model for demonstration.\")\n",
        "    model = SwinIR(embed_dim=60, depths=[4,4,4,4], num_heads=[4,4,4,4]).to(DEVICE)\n",
        "else:\n",
        "    print(f\"Loading model from {MODEL_PATH}...\")\n",
        "    model = SwinIR(embed_dim=60, depths=[4,4,4,4], num_heads=[4,4,4,4]).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.eval()\n",
        "    \n",
        "print(f\"Model Parameter Count: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Original Model Size: {os.path.getsize(MODEL_PATH) / (1024*1024):.2f} MB\" if MODEL_PATH else \"No checkpoint to measure\")\n",
        "\n",
        "def apply_pruning(model, amount=0.2):\n",
        "    print(f\"Applying {amount*100}% L1 Unstructured Pruning to Conv layers...\")\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "            prune.remove(module, 'weight') # Make pruning permanent\n",
        "    return model\n",
        "\n",
        "# Example usage (uncomment to apply):\n",
        "# model = apply_pruning(model, amount=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Post-Training Static Quantization (INT8)\n",
        "Reduces the model size by ~4x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quantize_model(model):\n",
        "    \"\"\"Quantize model to INT8. Requires representative calibration data.\"\"\"\n",
        "    model.eval()\n",
        "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "    model_prepared = torch.quantization.prepare(model)\n",
        "    \n",
        "    # Calibrate with representative data\n",
        "    # NOTE: You need to provide calibration data here\n",
        "    # Example:\n",
        "    # for i in range(10):\n",
        "    #     dummy_input = torch.randn(1, 1, 64, 64)\n",
        "    #     model_prepared(dummy_input)\n",
        "    \n",
        "    # For demonstration, we'll skip actual calibration\n",
        "    print(\"⚠️ WARNING: Calibration data not provided. Skipping quantization.\")\n",
        "    print(\"To properly quantize, uncomment calibration loop above with real data.\")\n",
        "    return model\n",
        "    \n",
        "    # Uncomment below when calibration data is ready:\n",
        "    # model_int8 = torch.quantization.convert(model_prepared)\n",
        "    # print(\"Model quantized to INT8.\")\n",
        "    # return model_int8\n",
        "\n",
        "# Example usage:\n",
        "# quantized_model = quantize_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. ONNX Export\n",
        "Exporting for OpenVINO, TensorRT, or ONNX Runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_onnx(model, output_path=\"../swinir_lensing.onnx\"):\n",
        "    \"\"\"Export model to ONNX format for deployment.\"\"\"\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(1, 1, 64, 64)\n",
        "    \n",
        "    try:\n",
        "        torch.onnx.export(model, \n",
        "                          dummy_input, \n",
        "                          output_path, \n",
        "                          export_params=True, \n",
        "                          opset_version=12, \n",
        "                          do_constant_folding=True,\n",
        "                          input_names=['input'], \n",
        "                          output_names=['output'],\n",
        "                          dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}})\n",
        "        print(f\"✅ Model exported to {output_path}\")\n",
        "        print(f\"   File size: {os.path.getsize(output_path) / (1024*1024):.2f} MB\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error exporting ONNX: {e}\")\n",
        "        print(\"   Some models may require trace mode or different opset versions.\")\n",
        "\n",
        "# Example usage:\n",
        "# export_onnx(model, output_path=\"../swinir_lensing.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Final Benchmarking\n",
        "Compare inference speed and file size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benchmarking logic ready.\n"
          ]
        }
      ],
      "source": [
        "def get_model_size(path):\n",
        "    \"\"\"Get model file size in MB.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"⚠️ File not found: {path}\")\n",
        "        return 0\n",
        "    size = os.path.getsize(path) / (1024 * 1024)\n",
        "    print(f\"Model Size: {size:.2f} MB\")\n",
        "    return size\n",
        "\n",
        "def benchmark_inference(model, num_iterations=100):\n",
        "    \"\"\"Benchmark model inference time.\"\"\"\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(1, 1, 64, 64).to(DEVICE)\n",
        "    \n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            _ = model(dummy_input)\n",
        "    \n",
        "    # Benchmark\n",
        "    if DEVICE == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_iterations):\n",
        "            _ = model(dummy_input)\n",
        "    \n",
        "    if DEVICE == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    avg_time = elapsed_time / num_iterations * 1000  # ms\n",
        "    \n",
        "    print(f\"Average inference time: {avg_time:.2f} ms ({num_iterations} iterations)\")\n",
        "    return avg_time\n",
        "\n",
        "print(\"Benchmarking logic ready.\")\n",
        "print(\"\\nExample usage:\")\n",
        "print(\"  - Compare model sizes: get_model_size(MODEL_PATH)\")\n",
        "print(\"  - Benchmark inference: benchmark_inference(model)\")\n",
        "print(\"  - Export to ONNX: export_onnx(model)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
