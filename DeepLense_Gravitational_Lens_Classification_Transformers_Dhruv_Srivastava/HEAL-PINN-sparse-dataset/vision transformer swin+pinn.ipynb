{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7Ekxmo1XV5I"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37268,
     "status": "ok",
     "timestamp": 1758703101532,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "_mFRKD8F__hl",
    "outputId": "975ec840-9641-4c82-8b36-6165eb48a3fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9777,
     "status": "ok",
     "timestamp": 1758703111314,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "vRM2kxST8KJZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Import dependencies\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1758703111368,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "9Dm6SC1o_txB"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyDatasetViT(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.class_names = ['axion', 'cdm', 'no_sub']\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Loading dataset from: {data_dir}\")\n",
    "        print(f\"Looking for classes: {self.class_names}\")\n",
    "\n",
    "        for idx, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            print(f\"--- Processing class: {class_name} ---\")\n",
    "\n",
    "            if not os.path.exists(class_dir):\n",
    "                print(f\"[ERROR] Directory not found: {class_dir}\")\n",
    "                continue\n",
    "\n",
    "            files = os.listdir(class_dir)\n",
    "\n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.npy'):\n",
    "                    file_path = os.path.join(class_dir, file_name)\n",
    "                    loaded_data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "                    if class_name == 'axion':\n",
    "                        image = loaded_data[0]\n",
    "                    else:\n",
    "                        image = loaded_data\n",
    "\n",
    "                    # [DEBUG] Print the shape of the raw numpy array\n",
    "                    print(f\"  [DEBUG] Loaded '{file_name}'. Raw numpy shape: {image.shape}\")\n",
    "\n",
    "                    # Ensure the image is a 2D array (H, W) before adding channel dimension.\n",
    "                    if image.ndim != 2:\n",
    "                        image = np.squeeze(image)\n",
    "\n",
    "                    # Convert to a float tensor and add a channel dimension -> [1, H, W]\n",
    "                    image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                    # [DEBUG] Print the shape of the final tensor being stored in the dataset\n",
    "                    print(f\"  [DEBUG] Storing tensor with final shape: {image_tensor.shape}\\n\")\n",
    "\n",
    "                    self.data.append(image_tensor)\n",
    "                    self.labels.append(idx)\n",
    "\n",
    "        print(\"\\n--- Dataset Loading Complete ---\")\n",
    "        print(f\"Total images loaded: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This method is called by the DataLoader to get one item from the dataset.\n",
    "        The debug prints here are CRITICAL for finding the error.\n",
    "        \"\"\"\n",
    "        #print(f\"--- Getting item index: {idx} ---\")\n",
    "\n",
    "        # Retrieve the pre-loaded tensor and its label\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # [DEBUG] Print shape BEFORE the transform is applied\n",
    "        #print(f\"  [DEBUG] Shape of tensor BEFORE transform: {image.shape}\")\n",
    "\n",
    "        # Apply transformations (e.g., resizing) if they are provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            # [DEBUG] Print shape AFTER the transform is applied\n",
    "            #print(f\"  [DEBUG] Shape of tensor AFTER transform: {image.shape}\")\n",
    "        else:\n",
    "            #print(\"  [DEBUG] No transform was applied.\")\n",
    "            pass\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1758703111378,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "VJEgowwT_txC",
    "outputId": "1e1ea89a-3c06-4103-8f99-9ee8e1d8eb61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Directory: /content/drive/MyDrive/Model_III_dataset/Model_III\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the transforms module\n",
    "from torchvision import transforms\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Data Directories\n",
    "train_dir = '/content/drive/MyDrive/Model_III_dataset/Model_III'\n",
    "#val_dir = '../dataset/dataset/val'\n",
    "\n",
    "print(f\"Training Directory: {train_dir}\")\n",
    "#print(f\"Validation Directory: {val_dir}\")\n",
    "\n",
    "vit_transforms = transforms.Compose([\n",
    "    # transforms.ToTensor(), # Removed ToTensor()\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90), # Can be any angle range\n",
    "])\n",
    "\n",
    "# Create Datasets and Dataloaders\n",
    "#train_dataset = MyDataset(train_dir)\n",
    "#val_dataset = MyDataset(val_dir)\n",
    "#dataset = MyDatasetViT(train_dir, vit_transforms)\n",
    "#train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.75, 0.15, 0.1])\n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "#print(f\"Batch Size: {batch_size}\")\n",
    "#print(f\"Number of Training Batches: {len(train_loader)}\")\n",
    "#print(f\"Number of Validation Batches: {len(val_loader)}\")\n",
    "\n",
    "#Save the dataloader so that we don't have to bear with this pain again\n",
    "#torch.save(train_loader, '/content/drive/MyDrive/Model_III_dataset/train_loader.pth')\n",
    "#torch.save(val_loader, '/content/drive/MyDrive/Model_III_dataset/val_loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 39905,
     "status": "ok",
     "timestamp": 1758703151285,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "gvqsjdgzKmOL"
   },
   "outputs": [],
   "source": [
    "#import data loaders from file\n",
    "train_loader = torch.load('/content/drive/MyDrive/train_loader.pth', weights_only=False)\n",
    "val_loader = torch.load('/content/drive/MyDrive/val_loader.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3568,
     "status": "ok",
     "timestamp": 1758703154850,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "y6Q9I1Hw_txD",
    "outputId": "dd980201-6599-4a8e-d9da-200bf20eaac3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.layers import DropPath, trunc_normal_\n",
    "from torch import Tensor\n",
    "\n",
    "# --- Helper Classes from Original Swin Transformer ---\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" Multi-Layer Perceptron \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Partitions the input tensor into non-overlapping windows.\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (tuple[int]): Window size (height, width).\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Reverses the window partitioning.\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (tuple[int]): Window size (height, width).\n",
    "        H (int): Height of image.\n",
    "        W (int): Width of image.\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size[0] / window_size[1]))\n",
    "    x = windows.view(\n",
    "        B, H // window_size[0], W // window_size[1], window_size[0], window_size[1], -1\n",
    "    )\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both shifted and non-shifted windows.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        window_size,\n",
    "        num_heads,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
    "        )\n",
    "\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.view(-1)\n",
    "        ].view(\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            -1,\n",
    "        )\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        num_heads,\n",
    "        window_size=(5, 5),\n",
    "        shift_size=(0, 0),\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        if min(self.input_resolution) <= min(self.window_size):\n",
    "            self.shift_size = (0, 0)\n",
    "            self.window_size = self.input_resolution\n",
    "\n",
    "        assert 0 <= self.shift_size[0] < self.window_size[0], \"shift_size must be less than window_size\"\n",
    "        assert 0 <= self.shift_size[1] < self.window_size[1], \"shift_size must be less than window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=self.window_size,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "        if self.shift_size[0] > 0 or self.shift_size[1] > 0:\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))\n",
    "            h_slices = (\n",
    "                slice(0, -self.window_size[0]),\n",
    "                slice(-self.window_size[0], -self.shift_size[0]),\n",
    "                slice(-self.shift_size[0], None),\n",
    "            )\n",
    "            w_slices = (\n",
    "                slice(0, -self.window_size[1]),\n",
    "                slice(-self.window_size[1], -self.shift_size[1]),\n",
    "                slice(-self.shift_size[1], None),\n",
    "            )\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)\n",
    "            mask_windows = mask_windows.view(-1, self.window_size[0] * self.window_size[1])\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n",
    "                attn_mask == 0, float(0.0)\n",
    "            )\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        if self.shift_size[0] > 0 or self.shift_size[1] > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size[0], -self.shift_size[1]), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size[0] * self.window_size[1], C)\n",
    "\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
    "\n",
    "        attn_windows = attn_windows.view(-1, self.window_size[0], self.window_size[1], C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "\n",
    "        if self.shift_size[0] > 0 or self.shift_size[1] > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size[0], self.shift_size[1]), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,\n",
    "                    input_resolution=input_resolution,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=window_size,\n",
    "                    shift_size=(0, 0) if (i % 2 == 0) else (window_size[0] // 2, window_size[1] // 2),\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=(150, 150), patch_size=(10, 10), in_chans=1, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.num_patches = self.patches_resolution[0] * self.patches_resolution[1]\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLSABlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Transformer block with Multi-Head Self-Attention and a Feed-Forward Network.\n",
    "    This is a placeholder for the block used in the RelativisticPhysicalInformedEncoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, num_patches, num_hidden_neurons, activation_function, device, dropout):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, num_hidden_neurons),\n",
    "            activation_function,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(num_hidden_neurons, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention part\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # MLP part\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class RelativisticPhysicalInformedEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module to perform inverse gravitational lensing using the Singular Isothermal Sphere (SIS) model.\n",
    "    \"\"\"\n",
    "    def __init__( self,\n",
    "                  image_size: int,\n",
    "                  patch_size: int,\n",
    "                  embed_dim: int,\n",
    "                  num_patches: int,\n",
    "                  num_heads: int,\n",
    "                  num_hidden_neurons: int,\n",
    "                  transformer_activation_function: nn.Module,\n",
    "                  num_transformer_blocks: int,\n",
    "                  dropout: float = 0.1,\n",
    "                  pixel_scale:float =0.101,\n",
    "                  k_min: float = 0.8,\n",
    "                  k_max: float = 1.2,\n",
    "                  eps: float = 1e-8\n",
    "                  ):\n",
    "        super(RelativisticPhysicalInformedEncoder, self).__init__()\n",
    "        self.pixel_scale = pixel_scale\n",
    "        self.profile_size = image_size\n",
    "        self.half_profile_size = self.profile_size // 2\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        self.transformer_activation_function = transformer_activation_function\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.k_min = k_min\n",
    "        self.k_max = k_max\n",
    "        self.eps = eps\n",
    "\n",
    "        self.transformer = nn.ModuleList()\n",
    "        self.num_neurons_flatten = (self.num_patches) * embed_dim # Removed +1 for CLS token as it's not used\n",
    "\n",
    "        for _ in range(num_transformer_blocks):\n",
    "            self.transformer.append(\n",
    "                TransformerLSABlock(embed_dim,\n",
    "                                    num_heads,\n",
    "                                    self.num_patches,\n",
    "                                    num_hidden_neurons,\n",
    "                                    transformer_activation_function,\n",
    "                                    None, # Device is handled in forward pass\n",
    "                                    dropout)\n",
    "            )\n",
    "\n",
    "        self.compressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.num_neurons_flatten, self.profile_size * self.profile_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Create coordinate grids, but don't move to device yet\n",
    "        x_coordinates = torch.linspace(-self.half_profile_size, self.half_profile_size - 1, self.profile_size) * self.pixel_scale\n",
    "        y_coordinates = torch.linspace(-self.half_profile_size, self.half_profile_size - 1, self.profile_size) * self.pixel_scale\n",
    "        grid_x, grid_y = torch.meshgrid(x_coordinates, y_coordinates, indexing='ij')\n",
    "        self.register_buffer('grid_x', grid_x)\n",
    "        self.register_buffer('grid_y', grid_y)\n",
    "\n",
    "\n",
    "    def forward(self, input_images: Tensor, patches: Tensor)->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the module.\n",
    "        \"\"\"\n",
    "        # Ensure input_images is 2D (B, H*W) if it's not already\n",
    "        if input_images.dim() == 3:\n",
    "            input_images = input_images.squeeze(1)\n",
    "\n",
    "        batch_size = input_images.shape[0]\n",
    "        device = input_images.device\n",
    "\n",
    "        # Move grid tensors to the correct device\n",
    "        flat_grid_x = self.grid_x.flatten().to(device)\n",
    "        flat_grid_y = self.grid_y.flatten().to(device)\n",
    "\n",
    "        # Pass patches through transformer blocks\n",
    "        k_transformed = patches\n",
    "        for layer in self.transformer:\n",
    "            k_transformed = layer(k_transformed)\n",
    "\n",
    "        # Generate k using the compressor\n",
    "        k_sigmoid = self.compressor(k_transformed)\n",
    "\n",
    "        k_sigmoid = k_sigmoid.view(-1, self.profile_size, self.profile_size)\n",
    "        k_sigmoid_flat = k_sigmoid.view(-1, self.profile_size * self.profile_size)\n",
    "        k_sigmoid_flat = self.k_min + (self.k_max - self.k_min) * k_sigmoid_flat\n",
    "\n",
    "        flat_input_images = input_images.view(batch_size, -1)\n",
    "        non_zero_mask = (flat_grid_x != 0) | (flat_grid_y != 0)\n",
    "        k_sigmoid_non_zero = k_sigmoid_flat[:, non_zero_mask]\n",
    "\n",
    "        shape_k_sigmoid_non_zero = k_sigmoid_non_zero.shape\n",
    "        k = k_sigmoid_non_zero.view(shape_k_sigmoid_non_zero[0], 1, 1, shape_k_sigmoid_non_zero[1])\n",
    "\n",
    "        non_zero_x = flat_grid_x[non_zero_mask]\n",
    "        non_zero_y = flat_grid_y[non_zero_mask]\n",
    "        non_zero_radius = torch.sqrt(non_zero_x ** 2 + non_zero_y ** 2)\n",
    "        non_zero_radius = non_zero_radius[None, None, None, :]\n",
    "\n",
    "        shifted_x = (non_zero_x[None, None, None, :] - k * non_zero_x[None, None, None, :] / (non_zero_radius + self.eps))\n",
    "        shifted_y = (non_zero_y[None, None, None, :] - k * non_zero_y[None, None, None, :] / (non_zero_radius + self.eps))\n",
    "\n",
    "        shifted_x_idx = torch.round(shifted_x / self.pixel_scale + self.half_profile_size).long()\n",
    "        shifted_y_idx = torch.round(shifted_y / self.pixel_scale + self.half_profile_size).long()\n",
    "\n",
    "        # Clamp indices to be within the valid range [0, profile_size-1]\n",
    "        shifted_x_idx.clamp_(0, self.profile_size - 1)\n",
    "        shifted_y_idx.clamp_(0, self.profile_size - 1)\n",
    "\n",
    "        output_images = torch.zeros(batch_size, self.profile_size, self.profile_size, device=device)\n",
    "        flat_output_images = output_images.view(batch_size, -1)\n",
    "\n",
    "        one_d_indices = shifted_x_idx * self.profile_size + shifted_y_idx\n",
    "        one_d_indices = one_d_indices.view(batch_size, -1)\n",
    "\n",
    "        input_values_at_original_positions = flat_input_images[:, non_zero_mask]\n",
    "\n",
    "        # Use scatter_add_ for safe parallel updates\n",
    "        # To handle multiple source pixels mapping to the same target pixel, we average them.\n",
    "        # We need a count tensor for averaging.\n",
    "        counts = torch.zeros_like(flat_output_images)\n",
    "        ones = torch.ones_like(input_values_at_original_positions)\n",
    "\n",
    "        flat_output_images.scatter_add_(1, one_d_indices, input_values_at_original_positions)\n",
    "        counts.scatter_add_(1, one_d_indices, ones)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        flat_output_images = torch.where(counts > 0, flat_output_images / counts, 0)\n",
    "\n",
    "        output_images = flat_output_images.view(batch_size, self.profile_size, self.profile_size)\n",
    "\n",
    "        max_values = torch.amax(output_images, dim=(1, 2), keepdim=True)\n",
    "        output_images = output_images / (max_values + self.eps)\n",
    "\n",
    "        return output_images\n",
    "\n",
    "\n",
    "class SwinTransformerForClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer with an optional Relativistic Physics Informed Encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=(150, 150), patch_size=(10, 10), in_chans=1, num_classes=3,\n",
    "                 embed_dim=96, depths=[2, 2, 6], num_heads=[3, 6, 12],\n",
    "                 window_size=(5, 5), mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_relativistic_encoder=False, relativistic_encoder_blocks=2, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim)\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.use_relativistic_encoder = use_relativistic_encoder\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        if self.use_relativistic_encoder:\n",
    "            self.relativistic_encoder = RelativisticPhysicalInformedEncoder(\n",
    "                image_size=img_size[0],\n",
    "                patch_size=patch_size[0],\n",
    "                embed_dim=embed_dim,\n",
    "                num_patches=num_patches,\n",
    "                num_heads=num_heads[0], # Use heads from the first Swin stage\n",
    "                num_hidden_neurons=int(embed_dim * mlp_ratio),\n",
    "                transformer_activation_function=nn.GELU(),\n",
    "                num_transformer_blocks=relativistic_encoder_blocks,\n",
    "                dropout=drop_rate\n",
    "            )\n",
    "\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        layer = BasicLayer(dim=int(embed_dim),\n",
    "                           input_resolution=patches_resolution,\n",
    "                           depth=sum(depths),\n",
    "                           num_heads=num_heads[0],\n",
    "                           window_size=window_size,\n",
    "                           mlp_ratio=self.mlp_ratio,\n",
    "                           qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                           drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                           drop_path=dpr,\n",
    "                           norm_layer=norm_layer,\n",
    "                           downsample=None)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.avgpool(x.transpose(1, 2))\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_relativistic_encoder:\n",
    "            # The input 'x' is expected to be (B, C, H, W)\n",
    "            # The relativistic encoder needs the initial patches for its internal transformer\n",
    "            initial_patches = self.patch_embed(x)\n",
    "            # The encoder also needs the 2D image (B, H, W)\n",
    "            x_encoded = self.relativistic_encoder(x.squeeze(1), initial_patches)\n",
    "            # Reshape the output to (B, 1, H, W) to proceed with feature extraction\n",
    "            x = x_encoded.unsqueeze(1)\n",
    "\n",
    "        # The rest of the forward pass proceeds from here\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758703154860,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "hivvAHcI41JJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#!pip install torch_xla[tpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758703154874,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "GtiRNfT34-6a"
   },
   "outputs": [],
   "source": [
    "#import torch_xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1758703154943,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "Q3rnNlt1_txE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "def plot_roc_curve(all_labels, all_probs, class_names):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve for each class and the micro/macro averages.\n",
    "    \"\"\"\n",
    "    # Binarize the labels for multi-class ROC analysis\n",
    "    all_labels_bin = label_binarize(all_labels, classes=range(len(class_names)))\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(len(class_names)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(all_labels_bin[:, i], all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels_bin.ravel(), all_probs.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(class_names))]))\n",
    "    # Then interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(len(class_names)):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= len(class_names)\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label=f'macro-average ROC curve (area = {roc_auc[\"macro\"]:.2f})',\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(len(class_names)), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of class {class_names[i]} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-class Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(\"/content/drive/MyDrive/Model_III_dataset/roc_curve_swin_pinn.png\")\n",
    "    print(\"\\nROC curve plot saved as roc_curve.png\")\n",
    "\n",
    "\"\"\"Training and Evaluation with Early Stopping\"\"\"\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, patience=2000):\n",
    "    \"\"\"\n",
    "    Trains the model with early stopping based on validation ROC AUC score.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to train.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n",
    "        criterion: The loss function.\n",
    "        optimizer: The optimization algorithm.\n",
    "        scheduler: The learning rate scheduler.\n",
    "        num_epochs (int): The maximum number of epochs to train for.\n",
    "        patience (int): Number of epochs to wait for improvement before stopping.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    best_roc_auc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_probs = []\n",
    "    best_labels = []\n",
    "\n",
    "    class_names = ['axion', 'cdm', 'no_sub']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"===== Epoch {epoch+1}/{num_epochs} =====\")\n",
    "\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # --- Calculate Metrics ---\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        # Calculate multi-class ROC AUC score\n",
    "        all_labels_np = np.array(all_labels)\n",
    "        all_probs_np = np.array(all_probs)\n",
    "        try:\n",
    "            val_roc_auc = roc_auc_score(all_labels_np, all_probs_np, multi_class='ovr', average='macro')\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not calculate ROC AUC: {e}\")\n",
    "            val_roc_auc = 0.0\n",
    "\n",
    "        # Epoch-level summary\n",
    "        #print(f'\\n[SUMMARY] Epoch {epoch+1}/{num_epochs}:')\n",
    "        #print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "        #print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val ROC AUC: {val_roc_auc:.4f}')\n",
    "\n",
    "        if val_roc_auc > best_roc_auc:\n",
    "            best_roc_auc = val_roc_auc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_probs = all_probs\n",
    "            best_labels = all_labels\n",
    "            #torch.save(model.state_dict(), '/content/drive/MyDrive/Model_III_dataset/lens_classifier_model_vision_transformer.pth')\n",
    "            print(f\"New best model saved with Val ROC AUC: {best_roc_auc:.4f}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement in Val ROC AUC for {epochs_no_improve} epoch(s). Best is {best_roc_auc:.4f}. Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {patience} epochs without improvement.\")\n",
    "            model.load_state_dict(best_model_wts)\n",
    "            break\n",
    "\n",
    "    print(\"\\nTraining Complete!\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # After the training loop, plot the ROC curve for the best model\n",
    "    if best_probs and best_labels:\n",
    "        plot_roc_curve(best_labels, best_probs, class_names)\n",
    "\n",
    "    return model, best_probs, best_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758703154958,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "9QBIRCLEkikD"
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), '/content/drive/MyDrive/Model_III_dataset/model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZJepKUw_txF",
    "outputId": "76291ff9-d31b-40d3-e3a5-01f0e8302d38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: Adam\n",
      "Learning Rate: 0.0001\n",
      "Training on device: cuda\n",
      "===== Epoch 1/2000 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with Val ROC AUC: 0.5015, Train Loss: 0.9679, Train Accuracy: 0.5034, Val Loss: 0.9620, Val Accuracy: 0.5186\n",
      "===== Epoch 2/2000 =====\n",
      "New best model saved with Val ROC AUC: 0.5015, Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 3/2000 =====\n",
      "New best model saved with Val ROC AUC: 0.5084, Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n",
      "===== Epoch 4/2000 =====\n",
      "No improvement in Val ROC AUC for 1 epoch(s). Best is 0.5084. Train Loss: 0.9652, Train Accuracy: 0.5069, Val Loss: 0.9625, Val Accuracy: 0.5186\n",
      "===== Epoch 5/2000 =====\n",
      "No improvement in Val ROC AUC for 2 epoch(s). Best is 0.5084. Train Loss: 0.9647, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 6/2000 =====\n",
      "No improvement in Val ROC AUC for 3 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 7/2000 =====\n",
      "No improvement in Val ROC AUC for 4 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9647, Val Accuracy: 0.5186\n",
      "===== Epoch 8/2000 =====\n",
      "No improvement in Val ROC AUC for 5 epoch(s). Best is 0.5084. Train Loss: 0.9647, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n",
      "===== Epoch 9/2000 =====\n",
      "No improvement in Val ROC AUC for 6 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n",
      "===== Epoch 10/2000 =====\n",
      "No improvement in Val ROC AUC for 7 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n",
      "===== Epoch 11/2000 =====\n",
      "No improvement in Val ROC AUC for 8 epoch(s). Best is 0.5084. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n",
      "===== Epoch 12/2000 =====\n",
      "No improvement in Val ROC AUC for 9 epoch(s). Best is 0.5084. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n",
      "===== Epoch 13/2000 =====\n",
      "No improvement in Val ROC AUC for 10 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n",
      "===== Epoch 14/2000 =====\n",
      "No improvement in Val ROC AUC for 11 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 15/2000 =====\n",
      "No improvement in Val ROC AUC for 12 epoch(s). Best is 0.5084. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n",
      "===== Epoch 16/2000 =====\n",
      "No improvement in Val ROC AUC for 13 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n",
      "===== Epoch 17/2000 =====\n",
      "No improvement in Val ROC AUC for 14 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n",
      "===== Epoch 18/2000 =====\n",
      "No improvement in Val ROC AUC for 15 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n",
      "===== Epoch 19/2000 =====\n",
      "No improvement in Val ROC AUC for 16 epoch(s). Best is 0.5084. Train Loss: 0.9646, Train Accuracy: 0.5069, Val Loss: 0.9610, Val Accuracy: 0.5186\n",
      "===== Epoch 20/2000 =====\n",
      "No improvement in Val ROC AUC for 17 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n",
      "===== Epoch 21/2000 =====\n",
      "No improvement in Val ROC AUC for 18 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 22/2000 =====\n",
      "No improvement in Val ROC AUC for 19 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n",
      "===== Epoch 23/2000 =====\n",
      "No improvement in Val ROC AUC for 20 epoch(s). Best is 0.5084. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 24/2000 =====\n",
      "No improvement in Val ROC AUC for 21 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n",
      "===== Epoch 25/2000 =====\n",
      "No improvement in Val ROC AUC for 22 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 26/2000 =====\n",
      "No improvement in Val ROC AUC for 23 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9633, Val Accuracy: 0.5186\n",
      "===== Epoch 27/2000 =====\n",
      "No improvement in Val ROC AUC for 24 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n",
      "===== Epoch 28/2000 =====\n",
      "No improvement in Val ROC AUC for 25 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n",
      "===== Epoch 29/2000 =====\n",
      "No improvement in Val ROC AUC for 26 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n",
      "===== Epoch 30/2000 =====\n",
      "No improvement in Val ROC AUC for 27 epoch(s). Best is 0.5084. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n",
      "===== Epoch 31/2000 =====\n",
      "No improvement in Val ROC AUC for 28 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n",
      "===== Epoch 32/2000 =====\n",
      "No improvement in Val ROC AUC for 29 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 33/2000 =====\n",
      "No improvement in Val ROC AUC for 30 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n",
      "===== Epoch 34/2000 =====\n",
      "No improvement in Val ROC AUC for 31 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n",
      "===== Epoch 35/2000 =====\n",
      "No improvement in Val ROC AUC for 32 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n",
      "===== Epoch 36/2000 =====\n",
      "No improvement in Val ROC AUC for 33 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n",
      "===== Epoch 37/2000 =====\n",
      "No improvement in Val ROC AUC for 34 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 38/2000 =====\n",
      "No improvement in Val ROC AUC for 35 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n",
      "===== Epoch 39/2000 =====\n",
      "No improvement in Val ROC AUC for 36 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 40/2000 =====\n",
      "No improvement in Val ROC AUC for 37 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 41/2000 =====\n",
      "No improvement in Val ROC AUC for 38 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n",
      "===== Epoch 42/2000 =====\n",
      "No improvement in Val ROC AUC for 39 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 43/2000 =====\n",
      "No improvement in Val ROC AUC for 40 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n",
      "===== Epoch 44/2000 =====\n",
      "No improvement in Val ROC AUC for 41 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n",
      "===== Epoch 45/2000 =====\n",
      "No improvement in Val ROC AUC for 42 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n",
      "===== Epoch 46/2000 =====\n",
      "No improvement in Val ROC AUC for 43 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 47/2000 =====\n",
      "No improvement in Val ROC AUC for 44 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n",
      "===== Epoch 48/2000 =====\n",
      "No improvement in Val ROC AUC for 45 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n",
      "===== Epoch 49/2000 =====\n",
      "No improvement in Val ROC AUC for 46 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 50/2000 =====\n",
      "No improvement in Val ROC AUC for 47 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n",
      "===== Epoch 51/2000 =====\n",
      "No improvement in Val ROC AUC for 48 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n",
      "===== Epoch 52/2000 =====\n",
      "No improvement in Val ROC AUC for 49 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 53/2000 =====\n",
      "No improvement in Val ROC AUC for 50 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n",
      "===== Epoch 54/2000 =====\n",
      "No improvement in Val ROC AUC for 51 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 55/2000 =====\n",
      "No improvement in Val ROC AUC for 52 epoch(s). Best is 0.5084. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9633, Val Accuracy: 0.5186\n",
      "===== Epoch 56/2000 =====\n",
      "No improvement in Val ROC AUC for 53 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n",
      "===== Epoch 57/2000 =====\n",
      "No improvement in Val ROC AUC for 54 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n",
      "===== Epoch 58/2000 =====\n",
      "No improvement in Val ROC AUC for 55 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 59/2000 =====\n",
      "No improvement in Val ROC AUC for 56 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n",
      "===== Epoch 60/2000 =====\n",
      "No improvement in Val ROC AUC for 57 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 61/2000 =====\n",
      "No improvement in Val ROC AUC for 58 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n",
      "===== Epoch 62/2000 =====\n",
      "No improvement in Val ROC AUC for 59 epoch(s). Best is 0.5084. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9627, Val Accuracy: 0.5186\n",
      "===== Epoch 63/2000 =====\n",
      "No improvement in Val ROC AUC for 60 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9627, Val Accuracy: 0.5186\n",
      "===== Epoch 64/2000 =====\n",
      "No improvement in Val ROC AUC for 61 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n",
      "===== Epoch 65/2000 =====\n",
      "No improvement in Val ROC AUC for 62 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n",
      "===== Epoch 66/2000 =====\n",
      "No improvement in Val ROC AUC for 63 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 67/2000 =====\n",
      "No improvement in Val ROC AUC for 64 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 68/2000 =====\n",
      "No improvement in Val ROC AUC for 65 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 69/2000 =====\n",
      "No improvement in Val ROC AUC for 66 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n",
      "===== Epoch 70/2000 =====\n",
      "No improvement in Val ROC AUC for 67 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n",
      "===== Epoch 71/2000 =====\n",
      "No improvement in Val ROC AUC for 68 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n",
      "===== Epoch 72/2000 =====\n",
      "No improvement in Val ROC AUC for 69 epoch(s). Best is 0.5084. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n",
      "===== Epoch 73/2000 =====\n",
      "No improvement in Val ROC AUC for 70 epoch(s). Best is 0.5084. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n",
      "===== Epoch 74/2000 =====\n",
      "No improvement in Val ROC AUC for 71 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n",
      "===== Epoch 75/2000 =====\n",
      "No improvement in Val ROC AUC for 72 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n",
      "===== Epoch 76/2000 =====\n",
      "No improvement in Val ROC AUC for 73 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 77/2000 =====\n",
      "No improvement in Val ROC AUC for 74 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n",
      "===== Epoch 78/2000 =====\n",
      "No improvement in Val ROC AUC for 75 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n",
      "===== Epoch 79/2000 =====\n",
      "No improvement in Val ROC AUC for 76 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n",
      "===== Epoch 80/2000 =====\n",
      "No improvement in Val ROC AUC for 77 epoch(s). Best is 0.5084. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n",
      "===== Epoch 81/2000 =====\n",
      "No improvement in Val ROC AUC for 78 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n",
      "===== Epoch 82/2000 =====\n",
      "No improvement in Val ROC AUC for 79 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n",
      "===== Epoch 83/2000 =====\n",
      "No improvement in Val ROC AUC for 80 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 84/2000 =====\n",
      "No improvement in Val ROC AUC for 81 epoch(s). Best is 0.5084. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 85/2000 =====\n",
      "No improvement in Val ROC AUC for 82 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 86/2000 =====\n",
      "No improvement in Val ROC AUC for 83 epoch(s). Best is 0.5084. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n",
      "===== Epoch 87/2000 =====\n",
      "No improvement in Val ROC AUC for 84 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n",
      "===== Epoch 88/2000 =====\n",
      "No improvement in Val ROC AUC for 85 epoch(s). Best is 0.5084. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n",
      "===== Epoch 89/2000 =====\n",
      "No improvement in Val ROC AUC for 86 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 90/2000 =====\n",
      "No improvement in Val ROC AUC for 87 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 91/2000 =====\n",
      "No improvement in Val ROC AUC for 88 epoch(s). Best is 0.5084. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n",
      "===== Epoch 92/2000 =====\n",
      "No improvement in Val ROC AUC for 89 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n",
      "===== Epoch 93/2000 =====\n",
      "No improvement in Val ROC AUC for 90 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 94/2000 =====\n",
      "No improvement in Val ROC AUC for 91 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n",
      "===== Epoch 95/2000 =====\n",
      "No improvement in Val ROC AUC for 92 epoch(s). Best is 0.5084. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n",
      "===== Epoch 96/2000 =====\n",
      "No improvement in Val ROC AUC for 93 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n",
      "===== Epoch 97/2000 =====\n",
      "No improvement in Val ROC AUC for 94 epoch(s). Best is 0.5084. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n",
      "===== Epoch 98/2000 =====\n",
      "No improvement in Val ROC AUC for 95 epoch(s). Best is 0.5084. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n",
      "===== Epoch 99/2000 =====\n",
      "No improvement in Val ROC AUC for 96 epoch(s). Best is 0.5084. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n",
      "===== Epoch 100/2000 =====\n",
      "No improvement in Val ROC AUC for 97 epoch(s). Best is 0.5084. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n",
      "===== Epoch 101/2000 =====\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "        image_size (int): Size of the input image (e.g., 224).\n",
    "        patch_size (int): Size of each patch (e.g., 16).\n",
    "        in_channels (int): Number of input channels (e.g., 1 for your task).\n",
    "        num_classes (int): Number of output classes (e.g., 3 for your task).\n",
    "        embed_dim (int): The main embedding dimension (e.g., 768 for ViT-Base).\n",
    "        depth (int): Number of Transformer Encoder blocks (e.g., 12 for ViT-Base).\n",
    "        num_heads (int): Number of attention heads (e.g., 12 for ViT-Base).\n",
    "        mlp_ratio (float): Ratio to determine MLP hidden dimension (e.g., 4.0).\n",
    "        dropout (float): Dropout probability.\n",
    "\"\"\"\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.05\n",
    "num_epochs = 2000\n",
    "warmup_epochs = 10\n",
    "model = SwinTransformerForClassification(\n",
    "        img_size=(64, 64),\n",
    "        patch_size=(10, 10),\n",
    "        in_chans=1,\n",
    "        num_classes=3,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 2],\n",
    "        num_heads=[3, 6, 12],\n",
    "        window_size=(3, 3), # Changed window size to 3x3\n",
    "        use_relativistic_encoder=True, # <-- Set to True\n",
    "        relativistic_encoder_blocks=2  # <-- Hyperparameter for the new encoder\n",
    "    )\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "def warmup_lambda(current_epoch):\n",
    "    if current_epoch < warmup_epochs:\n",
    "        return float(current_epoch) / float(max(1, warmup_epochs))\n",
    "    return 1.0\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "main_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs, eta_min=1e-6)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])\n",
    "\n",
    "\n",
    "print(\"Optimizer: Adam\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "\n",
    "# Train Model\n",
    "model, all_probs, all_labels = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_-lEQtu_txF"
   },
   "source": [
    "\"\"\" ROC Curve Plotting Function\"\"\"\n",
    "def plot_roc_curve(all_preds, all_labels):\n",
    "    print(\"Generating ROC Curve\")\n",
    "\n",
    "    # Convert predictions and labels to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    n_classes = 3\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve((all_labels == i).astype(int), all_preds[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        print(f\"Class {i} ROC AUC: {roc_auc[i]:.4f}\")\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    class_names = ['Axion', 'CDM', 'No Substructure']\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color,\n",
    "                 label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('/content/drive/MyDrive/Model_III_dataset/roc_curve_swin_pinn.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"ROC Curve saved as roc_curve.png\")\n",
    "\n",
    "\n",
    "plot_roc_curve(all_probs, all_labels)\n",
    "\n",
    "print(\"Training and Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 91655,
     "status": "aborted",
     "timestamp": 1758703155727,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "MZ59pFZa_txG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1wEko_Ik-lhlsW1rYndRXwX-Mgx9jST28",
     "timestamp": 1756243482189
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
