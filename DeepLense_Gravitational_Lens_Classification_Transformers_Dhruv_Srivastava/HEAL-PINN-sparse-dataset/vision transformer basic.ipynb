{"cells":[{"cell_type":"markdown","metadata":{"id":"X7Ekxmo1XV5I"},"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22890,"status":"ok","timestamp":1758069523537,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"_mFRKD8F__hl","outputId":"f6f60308-3c97-4fa2-af32-0c785e559e46"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11068,"status":"ok","timestamp":1758069534609,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"vRM2kxST8KJZ"},"outputs":[],"source":["\n","\"\"\"Import dependencies\"\"\"\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models import resnet18\n","from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1758069534616,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"9Dm6SC1o_txB"},"outputs":[],"source":["\"\"\"Define Dataset Class for Vision Transformer with Debugging\"\"\"\n","class MyDatasetViT(Dataset):\n","    def __init__(self, data_dir, transform=None):\n","        self.data = []\n","        self.labels = []\n","        self.class_names = ['axion', 'cdm', 'no_sub']\n","        self.transform = transform\n","\n","        print(f\"Loading dataset from: {data_dir}\")\n","        print(f\"Looking for classes: {self.class_names}\")\n","\n","        for idx, class_name in enumerate(self.class_names):\n","            class_dir = os.path.join(data_dir, class_name)\n","            print(f\"--- Processing class: {class_name} ---\")\n","\n","            if not os.path.exists(class_dir):\n","                print(f\"[ERROR] Directory not found: {class_dir}\")\n","                continue\n","\n","            files = os.listdir(class_dir)\n","\n","            for file_name in files:\n","                if file_name.endswith('.npy'):\n","                    file_path = os.path.join(class_dir, file_name)\n","                    loaded_data = np.load(file_path, allow_pickle=True)\n","\n","                    if class_name == 'axion':\n","                        image = loaded_data[0]\n","                    else:\n","                        image = loaded_data\n","\n","                    # [DEBUG] Print the shape of the raw numpy array\n","                    print(f\"  [DEBUG] Loaded '{file_name}'. Raw numpy shape: {image.shape}\")\n","\n","                    # Ensure the image is a 2D array (H, W) before adding channel dimension.\n","                    if image.ndim != 2:\n","                        image = np.squeeze(image)\n","\n","                    # Convert to a float tensor and add a channel dimension -\u003e [1, H, W]\n","                    image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n","\n","                    # [DEBUG] Print the shape of the final tensor being stored in the dataset\n","                    print(f\"  [DEBUG] Storing tensor with final shape: {image_tensor.shape}\\n\")\n","\n","                    self.data.append(image_tensor)\n","                    self.labels.append(idx)\n","\n","        print(\"\\n--- Dataset Loading Complete ---\")\n","        print(f\"Total images loaded: {len(self.data)}\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        This method is called by the DataLoader to get one item from the dataset.\n","        The debug prints here are CRITICAL for finding the error.\n","        \"\"\"\n","        #print(f\"--- Getting item index: {idx} ---\")\n","\n","        # Retrieve the pre-loaded tensor and its label\n","        image = self.data[idx]\n","        label = self.labels[idx]\n","\n","        # [DEBUG] Print shape BEFORE the transform is applied\n","        #print(f\"  [DEBUG] Shape of tensor BEFORE transform: {image.shape}\")\n","\n","        # Apply transformations (e.g., resizing) if they are provided\n","        if self.transform:\n","            image = self.transform(image)\n","            # [DEBUG] Print shape AFTER the transform is applied\n","            #print(f\"  [DEBUG] Shape of tensor AFTER transform: {image.shape}\")\n","        else:\n","            #print(\"  [DEBUG] No transform was applied.\")\n","            pass\n","\n","        return image, label"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1758069534636,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"VJEgowwT_txC","outputId":"2e687d3a-9096-4fcf-8961-6c3ce9d02bcb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Directory: /content/drive/MyDrive/Model_V/Model_V\n"]}],"source":["\n","# Import the transforms module\n","from torchvision import transforms\n","# Hyperparameters\n","batch_size = 32\n","learning_rate = 0.001\n","num_epochs = 100\n","\n","# Data Directories\n","train_dir = '/content/drive/MyDrive/Model_V/Model_V'\n","#val_dir = '../dataset/dataset/val'\n","\n","print(f\"Training Directory: {train_dir}\")\n","#print(f\"Validation Directory: {val_dir}\")\n","\n","vit_transforms = transforms.Compose([\n","    # transforms.ToTensor(), # Removed ToTensor()\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomVerticalFlip(p=0.5),\n","    transforms.RandomRotation(degrees=90), # Can be any angle range\n","])\n","\n","# Create Datasets and Dataloaders\n","#dataset = MyDatasetViT(train_dir, vit_transforms)\n","#train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.75, 0.15, 0.1])\n","\n","#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","#val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n","\n","#print(f\"Batch Size: {batch_size}\")\n","#print(f\"Number of Training Batches: {len(train_loader)}\")\n","#print(f\"Number of Validation Batches: {len(val_loader)}\")\n","\n","#Save the dataloader so that we don't have to bear with this pain again\n","#torch.save(train_loader, '/content/drive/MyDrive/Model_III_dataset/train_loader.pth')\n","#torch.save(val_loader, '/content/drive/MyDrive/Model_III_dataset/val_loader.pth')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":35443,"status":"ok","timestamp":1758069570081,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"gvqsjdgzKmOL"},"outputs":[],"source":["#import data loaders from file\n","train_loader = torch.load('/content/drive/MyDrive/train_loader.pth', weights_only=False)\n","val_loader = torch.load('/content/drive/MyDrive/val_loader.pth', weights_only=False)"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1758069845915,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"y6Q9I1Hw_txD"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n","        super().__init__()\n","        self.image_size = image_size\n","        self.patch_size = patch_size\n","        # Calculate num_patches based on the actual image and patch sizes\n","        self.num_patches = (image_size // patch_size) * (image_size // patch_size)\n","\n","        self.projection = nn.Conv2d(\n","            in_channels,\n","            embed_dim,\n","            kernel_size=patch_size,\n","            stride=patch_size\n","        )\n","\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n","        # Initialize positional embedding with the correct size\n","        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n","\n","    def forward(self, x):\n","        #print(f\"[DEBUG] Input shape to PatchEmbedding: {x.shape}\")\n","        x = self.projection(x)\n","        #print(f\"[DEBUG] Shape after projection: {x.shape}\")\n","        x = x.flatten(2)\n","        #print(f\"[DEBUG] Shape after flatten: {x.shape}\")\n","        x = x.transpose(1, 2)\n","        #print(f\"[DEBUG] Shape after transpose: {x.shape}\")\n","\n","\n","        batch_size = x.shape[0]\n","        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n","        #print(f\"[DEBUG] Shape of cls_tokens: {cls_tokens.shape}\")\n","\n","        x = torch.cat((cls_tokens, x), dim=1)\n","\n","        # Add debug prints here\n","        #print(f\"[DEBUG] Shape of x before adding positional embedding: {x.shape}\")\n","        #print(f\"[DEBUG] Shape of positional_embedding: {self.positional_embedding.shape}\")\n","\n","        x = x + self.positional_embedding\n","\n","        return x\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads, dropout=0.1):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n","\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.scale = self.head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n","        self.attn_dropout = nn.Dropout(dropout)\n","        self.proj = nn.Linear(embed_dim, embed_dim)\n","        self.proj_dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n","\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_dropout(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","\n","        x = self.proj(x)\n","        x = self.proj_dropout(x)\n","\n","        return x\n","\n","class MLP(nn.Module):\n","    def __init__(self, in_features, hidden_features, out_features, dropout=0.1):\n","        super().__init__()\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        x = self.dropout(x)\n","        return x\n","\n","class TransformerEncoderBlock(nn.Module):\n","    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n","        super().__init__()\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n","        self.mlp = MLP(in_features=embed_dim, hidden_features=mlp_hidden_dim, out_features=embed_dim, dropout=dropout)\n","\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.norm1(x))\n","        x = x + self.mlp(self.norm2(x))\n","        return x\n","\n","class VisionTransformer(nn.Module):\n","    def __init__(self, image_size=224, patch_size=16, in_channels=1, num_classes=3,\n","                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, dropout=0.1):\n","        super().__init__()\n","\n","        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n","\n","        self.encoder_blocks = nn.ModuleList([\n","            TransformerEncoderBlock(\n","                embed_dim=embed_dim,\n","                num_heads=num_heads,\n","                mlp_ratio=mlp_ratio,\n","                dropout=dropout,\n","            ) for i in range(depth)])\n","\n","        self.norm = nn.LayerNorm(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes)\n","\n","    def forward(self, x):\n","        x = self.patch_embed(x)\n","\n","        for block in self.encoder_blocks:\n","            x = block(x)\n","\n","        x = self.norm(x)\n","\n","        cls_token_final = x[:, 0]\n","        output = self.head(cls_token_final)\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"977fc062"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimizer: Adam\n","Learning Rate: 0.0001\n","Training on device: cuda\n","===== Epoch 1/2000 =====\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["New best model saved with Val ROC AUC: 0.4926, Train Loss: 0.9685, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 2/2000 =====\n","New best model saved with Val ROC AUC: 0.4946, Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 3/2000 =====\n","New best model saved with Val ROC AUC: 0.4992, Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 4/2000 =====\n","No improvement in Val ROC AUC for 1 epoch(s). Best is 0.4992. Train Loss: 0.9650, Train Accuracy: 0.5069, Val Loss: 0.9650, Val Accuracy: 0.5186\n","===== Epoch 5/2000 =====\n","No improvement in Val ROC AUC for 2 epoch(s). Best is 0.4992. Train Loss: 0.9647, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 6/2000 =====\n","No improvement in Val ROC AUC for 3 epoch(s). Best is 0.4992. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 7/2000 =====\n","New best model saved with Val ROC AUC: 0.5005, Train Loss: 0.9647, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 8/2000 =====\n","No improvement in Val ROC AUC for 1 epoch(s). Best is 0.5005. Train Loss: 0.9649, Train Accuracy: 0.5069, Val Loss: 0.9624, Val Accuracy: 0.5186\n","===== Epoch 9/2000 =====\n","No improvement in Val ROC AUC for 2 epoch(s). Best is 0.5005. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 10/2000 =====\n","No improvement in Val ROC AUC for 3 epoch(s). Best is 0.5005. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 11/2000 =====\n","New best model saved with Val ROC AUC: 0.5038, Train Loss: 0.9648, Train Accuracy: 0.5069, Val Loss: 0.9632, Val Accuracy: 0.5186\n","===== Epoch 12/2000 =====\n","No improvement in Val ROC AUC for 1 epoch(s). Best is 0.5038. Train Loss: 0.9648, Train Accuracy: 0.5069, Val Loss: 0.9633, Val Accuracy: 0.5186\n","===== Epoch 13/2000 =====\n","No improvement in Val ROC AUC for 2 epoch(s). Best is 0.5038. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 14/2000 =====\n","No improvement in Val ROC AUC for 3 epoch(s). Best is 0.5038. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 15/2000 =====\n","No improvement in Val ROC AUC for 4 epoch(s). Best is 0.5038. Train Loss: 0.9647, Train Accuracy: 0.5069, Val Loss: 0.9630, Val Accuracy: 0.5186\n","===== Epoch 16/2000 =====\n","No improvement in Val ROC AUC for 5 epoch(s). Best is 0.5038. Train Loss: 0.9646, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 17/2000 =====\n","New best model saved with Val ROC AUC: 0.5042, Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 18/2000 =====\n","No improvement in Val ROC AUC for 1 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 19/2000 =====\n","No improvement in Val ROC AUC for 2 epoch(s). Best is 0.5042. Train Loss: 0.9646, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n","===== Epoch 20/2000 =====\n","No improvement in Val ROC AUC for 3 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 21/2000 =====\n","No improvement in Val ROC AUC for 4 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 22/2000 =====\n","No improvement in Val ROC AUC for 5 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9629, Val Accuracy: 0.5186\n","===== Epoch 23/2000 =====\n","No improvement in Val ROC AUC for 6 epoch(s). Best is 0.5042. Train Loss: 0.9646, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 24/2000 =====\n","No improvement in Val ROC AUC for 7 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 25/2000 =====\n","No improvement in Val ROC AUC for 8 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 26/2000 =====\n","No improvement in Val ROC AUC for 9 epoch(s). Best is 0.5042. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 27/2000 =====\n","No improvement in Val ROC AUC for 10 epoch(s). Best is 0.5042. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 28/2000 =====\n","No improvement in Val ROC AUC for 11 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 29/2000 =====\n","No improvement in Val ROC AUC for 12 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9631, Val Accuracy: 0.5186\n","===== Epoch 30/2000 =====\n","No improvement in Val ROC AUC for 13 epoch(s). Best is 0.5042. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9634, Val Accuracy: 0.5186\n","===== Epoch 31/2000 =====\n","No improvement in Val ROC AUC for 14 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 32/2000 =====\n","No improvement in Val ROC AUC for 15 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 33/2000 =====\n","No improvement in Val ROC AUC for 16 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n","===== Epoch 34/2000 =====\n","No improvement in Val ROC AUC for 17 epoch(s). Best is 0.5042. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 35/2000 =====\n","No improvement in Val ROC AUC for 18 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 36/2000 =====\n","No improvement in Val ROC AUC for 19 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9624, Val Accuracy: 0.5186\n","===== Epoch 37/2000 =====\n","No improvement in Val ROC AUC for 20 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 38/2000 =====\n","No improvement in Val ROC AUC for 21 epoch(s). Best is 0.5042. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 39/2000 =====\n","No improvement in Val ROC AUC for 22 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 40/2000 =====\n","No improvement in Val ROC AUC for 23 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 41/2000 =====\n","No improvement in Val ROC AUC for 24 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 42/2000 =====\n","No improvement in Val ROC AUC for 25 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 43/2000 =====\n","No improvement in Val ROC AUC for 26 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 44/2000 =====\n","No improvement in Val ROC AUC for 27 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9625, Val Accuracy: 0.5186\n","===== Epoch 45/2000 =====\n","No improvement in Val ROC AUC for 28 epoch(s). Best is 0.5042. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 46/2000 =====\n","No improvement in Val ROC AUC for 29 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 47/2000 =====\n","No improvement in Val ROC AUC for 30 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 48/2000 =====\n","No improvement in Val ROC AUC for 31 epoch(s). Best is 0.5042. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 49/2000 =====\n","No improvement in Val ROC AUC for 32 epoch(s). Best is 0.5042. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 50/2000 =====\n","No improvement in Val ROC AUC for 33 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 51/2000 =====\n","No improvement in Val ROC AUC for 34 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 52/2000 =====\n","No improvement in Val ROC AUC for 35 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n","===== Epoch 53/2000 =====\n","No improvement in Val ROC AUC for 36 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 54/2000 =====\n","No improvement in Val ROC AUC for 37 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 55/2000 =====\n","No improvement in Val ROC AUC for 38 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 56/2000 =====\n","No improvement in Val ROC AUC for 39 epoch(s). Best is 0.5042. Train Loss: 0.9645, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 57/2000 =====\n","No improvement in Val ROC AUC for 40 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 58/2000 =====\n","No improvement in Val ROC AUC for 41 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 59/2000 =====\n","No improvement in Val ROC AUC for 42 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9624, Val Accuracy: 0.5186\n","===== Epoch 60/2000 =====\n","No improvement in Val ROC AUC for 43 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 61/2000 =====\n","No improvement in Val ROC AUC for 44 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 62/2000 =====\n","No improvement in Val ROC AUC for 45 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9610, Val Accuracy: 0.5186\n","===== Epoch 63/2000 =====\n","No improvement in Val ROC AUC for 46 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 64/2000 =====\n","No improvement in Val ROC AUC for 47 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 65/2000 =====\n","No improvement in Val ROC AUC for 48 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 66/2000 =====\n","No improvement in Val ROC AUC for 49 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 67/2000 =====\n","No improvement in Val ROC AUC for 50 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 68/2000 =====\n","No improvement in Val ROC AUC for 51 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 69/2000 =====\n","No improvement in Val ROC AUC for 52 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 70/2000 =====\n","No improvement in Val ROC AUC for 53 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 71/2000 =====\n","No improvement in Val ROC AUC for 54 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 72/2000 =====\n","No improvement in Val ROC AUC for 55 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 73/2000 =====\n","No improvement in Val ROC AUC for 56 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 74/2000 =====\n","No improvement in Val ROC AUC for 57 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 75/2000 =====\n","No improvement in Val ROC AUC for 58 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 76/2000 =====\n","No improvement in Val ROC AUC for 59 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 77/2000 =====\n","No improvement in Val ROC AUC for 60 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 78/2000 =====\n","No improvement in Val ROC AUC for 61 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9625, Val Accuracy: 0.5186\n","===== Epoch 79/2000 =====\n","No improvement in Val ROC AUC for 62 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 80/2000 =====\n","No improvement in Val ROC AUC for 63 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 81/2000 =====\n","No improvement in Val ROC AUC for 64 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 82/2000 =====\n","No improvement in Val ROC AUC for 65 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 83/2000 =====\n","No improvement in Val ROC AUC for 66 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 84/2000 =====\n","No improvement in Val ROC AUC for 67 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 85/2000 =====\n","No improvement in Val ROC AUC for 68 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 86/2000 =====\n","No improvement in Val ROC AUC for 69 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 87/2000 =====\n","No improvement in Val ROC AUC for 70 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 88/2000 =====\n","No improvement in Val ROC AUC for 71 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 89/2000 =====\n","No improvement in Val ROC AUC for 72 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 90/2000 =====\n","No improvement in Val ROC AUC for 73 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n","===== Epoch 91/2000 =====\n","No improvement in Val ROC AUC for 74 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 92/2000 =====\n","No improvement in Val ROC AUC for 75 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 93/2000 =====\n","No improvement in Val ROC AUC for 76 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9625, Val Accuracy: 0.5186\n","===== Epoch 94/2000 =====\n","No improvement in Val ROC AUC for 77 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 95/2000 =====\n","No improvement in Val ROC AUC for 78 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 96/2000 =====\n","No improvement in Val ROC AUC for 79 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 97/2000 =====\n","No improvement in Val ROC AUC for 80 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 98/2000 =====\n","No improvement in Val ROC AUC for 81 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 99/2000 =====\n","No improvement in Val ROC AUC for 82 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 100/2000 =====\n","No improvement in Val ROC AUC for 83 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 101/2000 =====\n","No improvement in Val ROC AUC for 84 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 102/2000 =====\n","No improvement in Val ROC AUC for 85 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 103/2000 =====\n","No improvement in Val ROC AUC for 86 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 104/2000 =====\n","No improvement in Val ROC AUC for 87 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 105/2000 =====\n","No improvement in Val ROC AUC for 88 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 106/2000 =====\n","No improvement in Val ROC AUC for 89 epoch(s). Best is 0.5042. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 107/2000 =====\n","No improvement in Val ROC AUC for 90 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n","===== Epoch 108/2000 =====\n","No improvement in Val ROC AUC for 91 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 109/2000 =====\n","No improvement in Val ROC AUC for 92 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 110/2000 =====\n","No improvement in Val ROC AUC for 93 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 111/2000 =====\n","No improvement in Val ROC AUC for 94 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 112/2000 =====\n","No improvement in Val ROC AUC for 95 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 113/2000 =====\n","No improvement in Val ROC AUC for 96 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 114/2000 =====\n","No improvement in Val ROC AUC for 97 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 115/2000 =====\n","No improvement in Val ROC AUC for 98 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 116/2000 =====\n","No improvement in Val ROC AUC for 99 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 117/2000 =====\n","No improvement in Val ROC AUC for 100 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 118/2000 =====\n","No improvement in Val ROC AUC for 101 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 119/2000 =====\n","No improvement in Val ROC AUC for 102 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 120/2000 =====\n","No improvement in Val ROC AUC for 103 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 121/2000 =====\n","No improvement in Val ROC AUC for 104 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 122/2000 =====\n","No improvement in Val ROC AUC for 105 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 123/2000 =====\n","No improvement in Val ROC AUC for 106 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 124/2000 =====\n","No improvement in Val ROC AUC for 107 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 125/2000 =====\n","No improvement in Val ROC AUC for 108 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 126/2000 =====\n","No improvement in Val ROC AUC for 109 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 127/2000 =====\n","No improvement in Val ROC AUC for 110 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 128/2000 =====\n","No improvement in Val ROC AUC for 111 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 129/2000 =====\n","No improvement in Val ROC AUC for 112 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 130/2000 =====\n","No improvement in Val ROC AUC for 113 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 131/2000 =====\n","No improvement in Val ROC AUC for 114 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 132/2000 =====\n","No improvement in Val ROC AUC for 115 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 133/2000 =====\n","No improvement in Val ROC AUC for 116 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n","===== Epoch 134/2000 =====\n","No improvement in Val ROC AUC for 117 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 135/2000 =====\n","No improvement in Val ROC AUC for 118 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 136/2000 =====\n","No improvement in Val ROC AUC for 119 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 137/2000 =====\n","No improvement in Val ROC AUC for 120 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 138/2000 =====\n","No improvement in Val ROC AUC for 121 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n","===== Epoch 139/2000 =====\n","No improvement in Val ROC AUC for 122 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 140/2000 =====\n","No improvement in Val ROC AUC for 123 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 141/2000 =====\n","No improvement in Val ROC AUC for 124 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 142/2000 =====\n","No improvement in Val ROC AUC for 125 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 143/2000 =====\n","No improvement in Val ROC AUC for 126 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 144/2000 =====\n","No improvement in Val ROC AUC for 127 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 145/2000 =====\n","No improvement in Val ROC AUC for 128 epoch(s). Best is 0.5042. Train Loss: 0.9644, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 146/2000 =====\n","No improvement in Val ROC AUC for 129 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 147/2000 =====\n","No improvement in Val ROC AUC for 130 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 148/2000 =====\n","No improvement in Val ROC AUC for 131 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 149/2000 =====\n","No improvement in Val ROC AUC for 132 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 150/2000 =====\n","No improvement in Val ROC AUC for 133 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 151/2000 =====\n","No improvement in Val ROC AUC for 134 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 152/2000 =====\n","No improvement in Val ROC AUC for 135 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 153/2000 =====\n","No improvement in Val ROC AUC for 136 epoch(s). Best is 0.5042. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 154/2000 =====\n","No improvement in Val ROC AUC for 137 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 155/2000 =====\n","No improvement in Val ROC AUC for 138 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 156/2000 =====\n","No improvement in Val ROC AUC for 139 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9638, Val Accuracy: 0.5186\n","===== Epoch 157/2000 =====\n","No improvement in Val ROC AUC for 140 epoch(s). Best is 0.5042. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 158/2000 =====\n","No improvement in Val ROC AUC for 141 epoch(s). Best is 0.5042. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 159/2000 =====\n","No improvement in Val ROC AUC for 142 epoch(s). Best is 0.5042. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n","===== Epoch 160/2000 =====\n","New best model saved with Val ROC AUC: 0.5051, Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 161/2000 =====\n","No improvement in Val ROC AUC for 1 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 162/2000 =====\n","No improvement in Val ROC AUC for 2 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 163/2000 =====\n","No improvement in Val ROC AUC for 3 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 164/2000 =====\n","No improvement in Val ROC AUC for 4 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 165/2000 =====\n","No improvement in Val ROC AUC for 5 epoch(s). Best is 0.5051. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 166/2000 =====\n","No improvement in Val ROC AUC for 6 epoch(s). Best is 0.5051. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 167/2000 =====\n","No improvement in Val ROC AUC for 7 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 168/2000 =====\n","No improvement in Val ROC AUC for 8 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 169/2000 =====\n","No improvement in Val ROC AUC for 9 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 170/2000 =====\n","No improvement in Val ROC AUC for 10 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 171/2000 =====\n","No improvement in Val ROC AUC for 11 epoch(s). Best is 0.5051. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 172/2000 =====\n","No improvement in Val ROC AUC for 12 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 173/2000 =====\n","No improvement in Val ROC AUC for 13 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 174/2000 =====\n","No improvement in Val ROC AUC for 14 epoch(s). Best is 0.5051. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 175/2000 =====\n","No improvement in Val ROC AUC for 15 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 176/2000 =====\n","No improvement in Val ROC AUC for 16 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 177/2000 =====\n","No improvement in Val ROC AUC for 17 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9638, Val Accuracy: 0.5186\n","===== Epoch 178/2000 =====\n","No improvement in Val ROC AUC for 18 epoch(s). Best is 0.5051. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 179/2000 =====\n","No improvement in Val ROC AUC for 19 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 180/2000 =====\n","No improvement in Val ROC AUC for 20 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 181/2000 =====\n","No improvement in Val ROC AUC for 21 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 182/2000 =====\n","No improvement in Val ROC AUC for 22 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 183/2000 =====\n","No improvement in Val ROC AUC for 23 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 184/2000 =====\n","No improvement in Val ROC AUC for 24 epoch(s). Best is 0.5051. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 185/2000 =====\n","No improvement in Val ROC AUC for 25 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 186/2000 =====\n","No improvement in Val ROC AUC for 26 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 187/2000 =====\n","No improvement in Val ROC AUC for 27 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 188/2000 =====\n","No improvement in Val ROC AUC for 28 epoch(s). Best is 0.5051. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 189/2000 =====\n","No improvement in Val ROC AUC for 29 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 190/2000 =====\n","No improvement in Val ROC AUC for 30 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 191/2000 =====\n","No improvement in Val ROC AUC for 31 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 192/2000 =====\n","No improvement in Val ROC AUC for 32 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 193/2000 =====\n","No improvement in Val ROC AUC for 33 epoch(s). Best is 0.5051. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 194/2000 =====\n","No improvement in Val ROC AUC for 34 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 195/2000 =====\n","No improvement in Val ROC AUC for 35 epoch(s). Best is 0.5051. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 196/2000 =====\n","No improvement in Val ROC AUC for 36 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 197/2000 =====\n","No improvement in Val ROC AUC for 37 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 198/2000 =====\n","No improvement in Val ROC AUC for 38 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 199/2000 =====\n","No improvement in Val ROC AUC for 39 epoch(s). Best is 0.5051. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9631, Val Accuracy: 0.5186\n","===== Epoch 200/2000 =====\n","No improvement in Val ROC AUC for 40 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 201/2000 =====\n","No improvement in Val ROC AUC for 41 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 202/2000 =====\n","No improvement in Val ROC AUC for 42 epoch(s). Best is 0.5051. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 203/2000 =====\n","No improvement in Val ROC AUC for 43 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 204/2000 =====\n","No improvement in Val ROC AUC for 44 epoch(s). Best is 0.5051. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 205/2000 =====\n","No improvement in Val ROC AUC for 45 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 206/2000 =====\n","No improvement in Val ROC AUC for 46 epoch(s). Best is 0.5051. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 207/2000 =====\n","No improvement in Val ROC AUC for 47 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 208/2000 =====\n","No improvement in Val ROC AUC for 48 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 209/2000 =====\n","No improvement in Val ROC AUC for 49 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 210/2000 =====\n","No improvement in Val ROC AUC for 50 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 211/2000 =====\n","No improvement in Val ROC AUC for 51 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 212/2000 =====\n","No improvement in Val ROC AUC for 52 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 213/2000 =====\n","No improvement in Val ROC AUC for 53 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 214/2000 =====\n","No improvement in Val ROC AUC for 54 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 215/2000 =====\n","No improvement in Val ROC AUC for 55 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 216/2000 =====\n","No improvement in Val ROC AUC for 56 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 217/2000 =====\n","No improvement in Val ROC AUC for 57 epoch(s). Best is 0.5051. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 218/2000 =====\n","No improvement in Val ROC AUC for 58 epoch(s). Best is 0.5051. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 219/2000 =====\n","No improvement in Val ROC AUC for 59 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 220/2000 =====\n","No improvement in Val ROC AUC for 60 epoch(s). Best is 0.5051. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 221/2000 =====\n","No improvement in Val ROC AUC for 61 epoch(s). Best is 0.5051. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 222/2000 =====\n","No improvement in Val ROC AUC for 62 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 223/2000 =====\n","No improvement in Val ROC AUC for 63 epoch(s). Best is 0.5051. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 224/2000 =====\n","No improvement in Val ROC AUC for 64 epoch(s). Best is 0.5051. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 225/2000 =====\n","New best model saved with Val ROC AUC: 0.5068, Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 226/2000 =====\n","No improvement in Val ROC AUC for 1 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9624, Val Accuracy: 0.5186\n","===== Epoch 227/2000 =====\n","No improvement in Val ROC AUC for 2 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 228/2000 =====\n","No improvement in Val ROC AUC for 3 epoch(s). Best is 0.5068. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 229/2000 =====\n","No improvement in Val ROC AUC for 4 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 230/2000 =====\n","No improvement in Val ROC AUC for 5 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 231/2000 =====\n","No improvement in Val ROC AUC for 6 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 232/2000 =====\n","No improvement in Val ROC AUC for 7 epoch(s). Best is 0.5068. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 233/2000 =====\n","No improvement in Val ROC AUC for 8 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 234/2000 =====\n","No improvement in Val ROC AUC for 9 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 235/2000 =====\n","No improvement in Val ROC AUC for 10 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 236/2000 =====\n","No improvement in Val ROC AUC for 11 epoch(s). Best is 0.5068. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 237/2000 =====\n","No improvement in Val ROC AUC for 12 epoch(s). Best is 0.5068. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 238/2000 =====\n","No improvement in Val ROC AUC for 13 epoch(s). Best is 0.5068. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 239/2000 =====\n","No improvement in Val ROC AUC for 14 epoch(s). Best is 0.5068. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 240/2000 =====\n","No improvement in Val ROC AUC for 15 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 241/2000 =====\n","No improvement in Val ROC AUC for 16 epoch(s). Best is 0.5068. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9625, Val Accuracy: 0.5186\n","===== Epoch 242/2000 =====\n","No improvement in Val ROC AUC for 17 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 243/2000 =====\n","No improvement in Val ROC AUC for 18 epoch(s). Best is 0.5068. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 244/2000 =====\n","No improvement in Val ROC AUC for 19 epoch(s). Best is 0.5068. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 245/2000 =====\n","No improvement in Val ROC AUC for 20 epoch(s). Best is 0.5068. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 246/2000 =====\n","No improvement in Val ROC AUC for 21 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 247/2000 =====\n","No improvement in Val ROC AUC for 22 epoch(s). Best is 0.5068. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9625, Val Accuracy: 0.5186\n","===== Epoch 248/2000 =====\n","No improvement in Val ROC AUC for 23 epoch(s). Best is 0.5068. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 249/2000 =====\n","No improvement in Val ROC AUC for 24 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n","===== Epoch 250/2000 =====\n","No improvement in Val ROC AUC for 25 epoch(s). Best is 0.5068. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 251/2000 =====\n","No improvement in Val ROC AUC for 26 epoch(s). Best is 0.5068. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9630, Val Accuracy: 0.5186\n","===== Epoch 252/2000 =====\n","No improvement in Val ROC AUC for 27 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 253/2000 =====\n","No improvement in Val ROC AUC for 28 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 254/2000 =====\n","No improvement in Val ROC AUC for 29 epoch(s). Best is 0.5068. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 255/2000 =====\n","No improvement in Val ROC AUC for 30 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9629, Val Accuracy: 0.5186\n","===== Epoch 256/2000 =====\n","No improvement in Val ROC AUC for 31 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9625, Val Accuracy: 0.5186\n","===== Epoch 257/2000 =====\n","No improvement in Val ROC AUC for 32 epoch(s). Best is 0.5068. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 258/2000 =====\n","No improvement in Val ROC AUC for 33 epoch(s). Best is 0.5068. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 259/2000 =====\n","No improvement in Val ROC AUC for 34 epoch(s). Best is 0.5068. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 260/2000 =====\n","No improvement in Val ROC AUC for 35 epoch(s). Best is 0.5068. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 261/2000 =====\n","No improvement in Val ROC AUC for 36 epoch(s). Best is 0.5068. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 262/2000 =====\n","No improvement in Val ROC AUC for 37 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 263/2000 =====\n","No improvement in Val ROC AUC for 38 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 264/2000 =====\n","No improvement in Val ROC AUC for 39 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 265/2000 =====\n","No improvement in Val ROC AUC for 40 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 266/2000 =====\n","No improvement in Val ROC AUC for 41 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 267/2000 =====\n","No improvement in Val ROC AUC for 42 epoch(s). Best is 0.5068. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 268/2000 =====\n","No improvement in Val ROC AUC for 43 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 269/2000 =====\n","No improvement in Val ROC AUC for 44 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 270/2000 =====\n","No improvement in Val ROC AUC for 45 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9650, Val Accuracy: 0.5186\n","===== Epoch 271/2000 =====\n","No improvement in Val ROC AUC for 46 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 272/2000 =====\n","No improvement in Val ROC AUC for 47 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 273/2000 =====\n","No improvement in Val ROC AUC for 48 epoch(s). Best is 0.5068. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 274/2000 =====\n","No improvement in Val ROC AUC for 49 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 275/2000 =====\n","No improvement in Val ROC AUC for 50 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 276/2000 =====\n","No improvement in Val ROC AUC for 51 epoch(s). Best is 0.5068. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 277/2000 =====\n","No improvement in Val ROC AUC for 52 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 278/2000 =====\n","No improvement in Val ROC AUC for 53 epoch(s). Best is 0.5068. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 279/2000 =====\n","No improvement in Val ROC AUC for 54 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 280/2000 =====\n","No improvement in Val ROC AUC for 55 epoch(s). Best is 0.5068. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 281/2000 =====\n","No improvement in Val ROC AUC for 56 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9626, Val Accuracy: 0.5186\n","===== Epoch 282/2000 =====\n","No improvement in Val ROC AUC for 57 epoch(s). Best is 0.5068. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 283/2000 =====\n","No improvement in Val ROC AUC for 58 epoch(s). Best is 0.5068. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 284/2000 =====\n","No improvement in Val ROC AUC for 59 epoch(s). Best is 0.5068. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 285/2000 =====\n","No improvement in Val ROC AUC for 60 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 286/2000 =====\n","No improvement in Val ROC AUC for 61 epoch(s). Best is 0.5068. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 287/2000 =====\n","New best model saved with Val ROC AUC: 0.5073, Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 288/2000 =====\n","No improvement in Val ROC AUC for 1 epoch(s). Best is 0.5073. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 289/2000 =====\n","No improvement in Val ROC AUC for 2 epoch(s). Best is 0.5073. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 290/2000 =====\n","No improvement in Val ROC AUC for 3 epoch(s). Best is 0.5073. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 291/2000 =====\n","No improvement in Val ROC AUC for 4 epoch(s). Best is 0.5073. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 292/2000 =====\n","No improvement in Val ROC AUC for 5 epoch(s). Best is 0.5073. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 293/2000 =====\n","No improvement in Val ROC AUC for 6 epoch(s). Best is 0.5073. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 294/2000 =====\n","No improvement in Val ROC AUC for 7 epoch(s). Best is 0.5073. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 295/2000 =====\n","No improvement in Val ROC AUC for 8 epoch(s). Best is 0.5073. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 296/2000 =====\n","No improvement in Val ROC AUC for 9 epoch(s). Best is 0.5073. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 297/2000 =====\n","No improvement in Val ROC AUC for 10 epoch(s). Best is 0.5073. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 298/2000 =====\n","No improvement in Val ROC AUC for 11 epoch(s). Best is 0.5073. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 299/2000 =====\n","No improvement in Val ROC AUC for 12 epoch(s). Best is 0.5073. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 300/2000 =====\n","No improvement in Val ROC AUC for 13 epoch(s). Best is 0.5073. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 301/2000 =====\n","No improvement in Val ROC AUC for 14 epoch(s). Best is 0.5073. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 302/2000 =====\n","No improvement in Val ROC AUC for 15 epoch(s). Best is 0.5073. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 303/2000 =====\n","No improvement in Val ROC AUC for 16 epoch(s). Best is 0.5073. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 304/2000 =====\n","No improvement in Val ROC AUC for 17 epoch(s). Best is 0.5073. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 305/2000 =====\n","No improvement in Val ROC AUC for 18 epoch(s). Best is 0.5073. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 306/2000 =====\n","New best model saved with Val ROC AUC: 0.5081, Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9611, Val Accuracy: 0.5186\n","===== Epoch 307/2000 =====\n","No improvement in Val ROC AUC for 1 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 308/2000 =====\n","No improvement in Val ROC AUC for 2 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 309/2000 =====\n","No improvement in Val ROC AUC for 3 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 310/2000 =====\n","No improvement in Val ROC AUC for 4 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9628, Val Accuracy: 0.5186\n","===== Epoch 311/2000 =====\n","No improvement in Val ROC AUC for 5 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9627, Val Accuracy: 0.5186\n","===== Epoch 312/2000 =====\n","No improvement in Val ROC AUC for 6 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 313/2000 =====\n","No improvement in Val ROC AUC for 7 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 314/2000 =====\n","No improvement in Val ROC AUC for 8 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 315/2000 =====\n","No improvement in Val ROC AUC for 9 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 316/2000 =====\n","No improvement in Val ROC AUC for 10 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 317/2000 =====\n","No improvement in Val ROC AUC for 11 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 318/2000 =====\n","No improvement in Val ROC AUC for 12 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9612, Val Accuracy: 0.5186\n","===== Epoch 319/2000 =====\n","No improvement in Val ROC AUC for 13 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 320/2000 =====\n","No improvement in Val ROC AUC for 14 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 321/2000 =====\n","No improvement in Val ROC AUC for 15 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 322/2000 =====\n","No improvement in Val ROC AUC for 16 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 323/2000 =====\n","No improvement in Val ROC AUC for 17 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 324/2000 =====\n","No improvement in Val ROC AUC for 18 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 325/2000 =====\n","No improvement in Val ROC AUC for 19 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9626, Val Accuracy: 0.5186\n","===== Epoch 326/2000 =====\n","No improvement in Val ROC AUC for 20 epoch(s). Best is 0.5081. Train Loss: 0.9643, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 327/2000 =====\n","No improvement in Val ROC AUC for 21 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 328/2000 =====\n","No improvement in Val ROC AUC for 22 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 329/2000 =====\n","No improvement in Val ROC AUC for 23 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 330/2000 =====\n","No improvement in Val ROC AUC for 24 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 331/2000 =====\n","No improvement in Val ROC AUC for 25 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 332/2000 =====\n","No improvement in Val ROC AUC for 26 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 333/2000 =====\n","No improvement in Val ROC AUC for 27 epoch(s). Best is 0.5081. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9610, Val Accuracy: 0.5186\n","===== Epoch 334/2000 =====\n","No improvement in Val ROC AUC for 28 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 335/2000 =====\n","No improvement in Val ROC AUC for 29 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 336/2000 =====\n","No improvement in Val ROC AUC for 30 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 337/2000 =====\n","No improvement in Val ROC AUC for 31 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 338/2000 =====\n","No improvement in Val ROC AUC for 32 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 339/2000 =====\n","No improvement in Val ROC AUC for 33 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 340/2000 =====\n","No improvement in Val ROC AUC for 34 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 341/2000 =====\n","No improvement in Val ROC AUC for 35 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 342/2000 =====\n","No improvement in Val ROC AUC for 36 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 343/2000 =====\n","No improvement in Val ROC AUC for 37 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 344/2000 =====\n","No improvement in Val ROC AUC for 38 epoch(s). Best is 0.5081. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 345/2000 =====\n","No improvement in Val ROC AUC for 39 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 346/2000 =====\n","No improvement in Val ROC AUC for 40 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 347/2000 =====\n","No improvement in Val ROC AUC for 41 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9624, Val Accuracy: 0.5186\n","===== Epoch 348/2000 =====\n","No improvement in Val ROC AUC for 42 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9634, Val Accuracy: 0.5186\n","===== Epoch 349/2000 =====\n","No improvement in Val ROC AUC for 43 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 350/2000 =====\n","No improvement in Val ROC AUC for 44 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 351/2000 =====\n","No improvement in Val ROC AUC for 45 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 352/2000 =====\n","No improvement in Val ROC AUC for 46 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 353/2000 =====\n","No improvement in Val ROC AUC for 47 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 354/2000 =====\n","No improvement in Val ROC AUC for 48 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 355/2000 =====\n","No improvement in Val ROC AUC for 49 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 356/2000 =====\n","No improvement in Val ROC AUC for 50 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9627, Val Accuracy: 0.5186\n","===== Epoch 357/2000 =====\n","No improvement in Val ROC AUC for 51 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 358/2000 =====\n","No improvement in Val ROC AUC for 52 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 359/2000 =====\n","No improvement in Val ROC AUC for 53 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 360/2000 =====\n","No improvement in Val ROC AUC for 54 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 361/2000 =====\n","No improvement in Val ROC AUC for 55 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 362/2000 =====\n","No improvement in Val ROC AUC for 56 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 363/2000 =====\n","No improvement in Val ROC AUC for 57 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9627, Val Accuracy: 0.5186\n","===== Epoch 364/2000 =====\n","No improvement in Val ROC AUC for 58 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 365/2000 =====\n","No improvement in Val ROC AUC for 59 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 366/2000 =====\n","No improvement in Val ROC AUC for 60 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 367/2000 =====\n","No improvement in Val ROC AUC for 61 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 368/2000 =====\n","No improvement in Val ROC AUC for 62 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 369/2000 =====\n","No improvement in Val ROC AUC for 63 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 370/2000 =====\n","No improvement in Val ROC AUC for 64 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9625, Val Accuracy: 0.5186\n","===== Epoch 371/2000 =====\n","No improvement in Val ROC AUC for 65 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 372/2000 =====\n","No improvement in Val ROC AUC for 66 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 373/2000 =====\n","No improvement in Val ROC AUC for 67 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 374/2000 =====\n","No improvement in Val ROC AUC for 68 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9627, Val Accuracy: 0.5186\n","===== Epoch 375/2000 =====\n","No improvement in Val ROC AUC for 69 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 376/2000 =====\n","No improvement in Val ROC AUC for 70 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 377/2000 =====\n","No improvement in Val ROC AUC for 71 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9624, Val Accuracy: 0.5186\n","===== Epoch 378/2000 =====\n","No improvement in Val ROC AUC for 72 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 379/2000 =====\n","No improvement in Val ROC AUC for 73 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 380/2000 =====\n","No improvement in Val ROC AUC for 74 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n","===== Epoch 381/2000 =====\n","No improvement in Val ROC AUC for 75 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9633, Val Accuracy: 0.5186\n","===== Epoch 382/2000 =====\n","No improvement in Val ROC AUC for 76 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 383/2000 =====\n","No improvement in Val ROC AUC for 77 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 384/2000 =====\n","No improvement in Val ROC AUC for 78 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 385/2000 =====\n","No improvement in Val ROC AUC for 79 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 386/2000 =====\n","No improvement in Val ROC AUC for 80 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 387/2000 =====\n","No improvement in Val ROC AUC for 81 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 388/2000 =====\n","No improvement in Val ROC AUC for 82 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 389/2000 =====\n","No improvement in Val ROC AUC for 83 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9636, Val Accuracy: 0.5186\n","===== Epoch 390/2000 =====\n","No improvement in Val ROC AUC for 84 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 391/2000 =====\n","No improvement in Val ROC AUC for 85 epoch(s). Best is 0.5081. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 392/2000 =====\n","No improvement in Val ROC AUC for 86 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 393/2000 =====\n","No improvement in Val ROC AUC for 87 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 394/2000 =====\n","No improvement in Val ROC AUC for 88 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 395/2000 =====\n","No improvement in Val ROC AUC for 89 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 396/2000 =====\n","No improvement in Val ROC AUC for 90 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 397/2000 =====\n","No improvement in Val ROC AUC for 91 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 398/2000 =====\n","No improvement in Val ROC AUC for 92 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 399/2000 =====\n","No improvement in Val ROC AUC for 93 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 400/2000 =====\n","No improvement in Val ROC AUC for 94 epoch(s). Best is 0.5081. Train Loss: 0.9642, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 401/2000 =====\n","No improvement in Val ROC AUC for 95 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 402/2000 =====\n","No improvement in Val ROC AUC for 96 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 403/2000 =====\n","No improvement in Val ROC AUC for 97 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9613, Val Accuracy: 0.5186\n","===== Epoch 404/2000 =====\n","No improvement in Val ROC AUC for 98 epoch(s). Best is 0.5081. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 405/2000 =====\n","No improvement in Val ROC AUC for 99 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 406/2000 =====\n","No improvement in Val ROC AUC for 100 epoch(s). Best is 0.5081. Train Loss: 0.9636, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 407/2000 =====\n","No improvement in Val ROC AUC for 101 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 408/2000 =====\n","No improvement in Val ROC AUC for 102 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 409/2000 =====\n","No improvement in Val ROC AUC for 103 epoch(s). Best is 0.5081. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 410/2000 =====\n","No improvement in Val ROC AUC for 104 epoch(s). Best is 0.5081. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9624, Val Accuracy: 0.5186\n","===== Epoch 411/2000 =====\n","No improvement in Val ROC AUC for 105 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 412/2000 =====\n","No improvement in Val ROC AUC for 106 epoch(s). Best is 0.5081. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 413/2000 =====\n","New best model saved with Val ROC AUC: 0.5083, Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 414/2000 =====\n","No improvement in Val ROC AUC for 1 epoch(s). Best is 0.5083. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 415/2000 =====\n","No improvement in Val ROC AUC for 2 epoch(s). Best is 0.5083. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 416/2000 =====\n","No improvement in Val ROC AUC for 3 epoch(s). Best is 0.5083. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 417/2000 =====\n","No improvement in Val ROC AUC for 4 epoch(s). Best is 0.5083. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 418/2000 =====\n","No improvement in Val ROC AUC for 5 epoch(s). Best is 0.5083. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 419/2000 =====\n","No improvement in Val ROC AUC for 6 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9626, Val Accuracy: 0.5186\n","===== Epoch 420/2000 =====\n","No improvement in Val ROC AUC for 7 epoch(s). Best is 0.5083. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 421/2000 =====\n","No improvement in Val ROC AUC for 8 epoch(s). Best is 0.5083. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 422/2000 =====\n","No improvement in Val ROC AUC for 9 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9634, Val Accuracy: 0.5186\n","===== Epoch 423/2000 =====\n","No improvement in Val ROC AUC for 10 epoch(s). Best is 0.5083. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 424/2000 =====\n","No improvement in Val ROC AUC for 11 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 425/2000 =====\n","No improvement in Val ROC AUC for 12 epoch(s). Best is 0.5083. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9625, Val Accuracy: 0.5186\n","===== Epoch 426/2000 =====\n","No improvement in Val ROC AUC for 13 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 427/2000 =====\n","No improvement in Val ROC AUC for 14 epoch(s). Best is 0.5083. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 428/2000 =====\n","No improvement in Val ROC AUC for 15 epoch(s). Best is 0.5083. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 429/2000 =====\n","No improvement in Val ROC AUC for 16 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 430/2000 =====\n","No improvement in Val ROC AUC for 17 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 431/2000 =====\n","No improvement in Val ROC AUC for 18 epoch(s). Best is 0.5083. Train Loss: 0.9636, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 432/2000 =====\n","No improvement in Val ROC AUC for 19 epoch(s). Best is 0.5083. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 433/2000 =====\n","No improvement in Val ROC AUC for 20 epoch(s). Best is 0.5083. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9621, Val Accuracy: 0.5186\n","===== Epoch 434/2000 =====\n","No improvement in Val ROC AUC for 21 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 435/2000 =====\n","No improvement in Val ROC AUC for 22 epoch(s). Best is 0.5083. Train Loss: 0.9636, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 436/2000 =====\n","No improvement in Val ROC AUC for 23 epoch(s). Best is 0.5083. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 437/2000 =====\n","No improvement in Val ROC AUC for 24 epoch(s). Best is 0.5083. Train Loss: 0.9641, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 438/2000 =====\n","No improvement in Val ROC AUC for 25 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 439/2000 =====\n","No improvement in Val ROC AUC for 26 epoch(s). Best is 0.5083. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9616, Val Accuracy: 0.5186\n","===== Epoch 440/2000 =====\n","No improvement in Val ROC AUC for 27 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 441/2000 =====\n","No improvement in Val ROC AUC for 28 epoch(s). Best is 0.5083. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9617, Val Accuracy: 0.5186\n","===== Epoch 442/2000 =====\n","No improvement in Val ROC AUC for 29 epoch(s). Best is 0.5083. Train Loss: 0.9640, Train Accuracy: 0.5069, Val Loss: 0.9619, Val Accuracy: 0.5186\n","===== Epoch 443/2000 =====\n","No improvement in Val ROC AUC for 30 epoch(s). Best is 0.5083. Train Loss: 0.9635, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 444/2000 =====\n","No improvement in Val ROC AUC for 31 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 445/2000 =====\n","No improvement in Val ROC AUC for 32 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9618, Val Accuracy: 0.5186\n","===== Epoch 446/2000 =====\n","No improvement in Val ROC AUC for 33 epoch(s). Best is 0.5083. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 447/2000 =====\n","No improvement in Val ROC AUC for 34 epoch(s). Best is 0.5083. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9615, Val Accuracy: 0.5186\n","===== Epoch 448/2000 =====\n","No improvement in Val ROC AUC for 35 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9614, Val Accuracy: 0.5186\n","===== Epoch 449/2000 =====\n","No improvement in Val ROC AUC for 36 epoch(s). Best is 0.5083. Train Loss: 0.9638, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 450/2000 =====\n","No improvement in Val ROC AUC for 37 epoch(s). Best is 0.5083. Train Loss: 0.9637, Train Accuracy: 0.5069, Val Loss: 0.9620, Val Accuracy: 0.5186\n","===== Epoch 451/2000 =====\n","No improvement in Val ROC AUC for 38 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9622, Val Accuracy: 0.5186\n","===== Epoch 452/2000 =====\n","No improvement in Val ROC AUC for 39 epoch(s). Best is 0.5083. Train Loss: 0.9639, Train Accuracy: 0.5069, Val Loss: 0.9623, Val Accuracy: 0.5186\n","===== Epoch 453/2000 =====\n"]}],"source":["\"\"\"\n","Args:\n","        image_size (int): Size of the input image (e.g., 64).\n","        patch_size (int): Size of each patch (e.g., 8).\n","        in_channels (int): Number of input channels (e.g., 1 for your task).\n","        num_classes (int): Number of output classes (e.g., 3 for your task).\n","        embed_dim (int): The main embedding dimension (e.g., 768 for ViT-Base).\n","        depth (int): Number of Transformer Encoder blocks (e.g., 12 for ViT-Base).\n","        num_heads (int): Number of attention heads (e.g., 12 for ViT-Base).\n","        mlp_ratio (float): Ratio to determine MLP hidden dimension (e.g., 4.0).\n","        dropout (float): Dropout probability.\n","\"\"\"\n","from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n","batch_size = 32\n","learning_rate = 1e-4\n","weight_decay = 0.05\n","num_epochs = 2000\n","warmup_epochs = 10\n","model = VisionTransformer(\n","        image_size=64, patch_size=8, in_channels=1, num_classes=3,\n","                 embed_dim=48, depth=2, num_heads=12, mlp_ratio=4.0, dropout=0.0\n","    )\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","\n","\n","#scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n","def warmup_lambda(current_epoch):\n","    if current_epoch \u003c warmup_epochs:\n","        return float(current_epoch) / float(max(1, warmup_epochs))\n","    return 1.0\n","warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n","main_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs, eta_min=1e-6)\n","scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])\n","\n","\n","print(\"Optimizer: Adam\")\n","print(f\"Learning Rate: {learning_rate}\")\n","\n","# Train Model\n","model, all_probs, all_labels = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1758069715273,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"hivvAHcI41JJ"},"outputs":[],"source":["\n","\n","#!pip install torch_xla[tpu]"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1758069715350,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"GtiRNfT34-6a"},"outputs":[],"source":["#import torch_xla"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":278,"status":"error","timestamp":1758069881508,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"ed08c68e","outputId":"c8e4228d-29f4-448c-8055-05f1b430d6be"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimizer: Adam\n","Learning Rate: 0.0001\n","Training on device: cuda\n","===== Epoch 1/2000 =====\n"]},{"ename":"RuntimeError","evalue":"The size of tensor a (37) must match the size of tensor b (226) at non-singleton dimension 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2465058945.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 43\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-898666820.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 108\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-97877416.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 130\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-97877416.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#print(f\"[DEBUG] Shape of positional_embedding: {self.positional_embedding.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (37) must match the size of tensor b (226) at non-singleton dimension 1"]}],"source":["\"\"\"\n","Args:\n","        image_size (int): Size of the input image (e.g., 224).\n","        patch_size (int): Size of each patch (e.g., 16).\n","        in_channels (int): Number of input channels (e.g., 1 for your task).\n","        num_classes (int): Number of output classes (e.g., 3 for your task).\n","        embed_dim (int): The main embedding dimension (e.g., 768 for ViT-Base).\n","        depth (int): Number of Transformer Encoder blocks (e.g., 12 for ViT-Base).\n","        num_heads (int): Number of attention heads (e.g., 12 for ViT-Base).\n","        mlp_ratio (float): Ratio to determine MLP hidden dimension (e.g., 4.0).\n","        dropout (float): Dropout probability.\n","\"\"\"\n","from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n","batch_size = 32\n","learning_rate = 1e-4\n","weight_decay = 0.05\n","num_epochs = 2000\n","warmup_epochs = 10\n","model = VisionTransformer(\n","        image_size=150, patch_size=10, in_channels=1, num_classes=3,\n","                 embed_dim=48, depth=2, num_heads=12, mlp_ratio=4.0, dropout=0.0\n","    )\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","\n","\n","#scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n","def warmup_lambda(current_epoch):\n","    if current_epoch \u003c warmup_epochs:\n","        return float(current_epoch) / float(max(1, warmup_epochs))\n","    return 1.0\n","warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n","main_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs, eta_min=1e-6)\n","scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])\n","\n","\n","print(\"Optimizer: Adam\")\n","print(f\"Learning Rate: {learning_rate}\")\n","\n","# Train Model\n","model, all_probs, all_labels = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"elapsed":305,"status":"error","timestamp":1758069738943,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"84d7eb50","outputId":"7ee4ece9-8acc-45d0-b8ae-b6da42a152ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimizer: Adam\n","Learning Rate: 0.0001\n","Training on device: cuda\n","===== Epoch 1/2000 =====\n","[DEBUG] Shape of x before adding positional embedding: torch.Size([32, 37, 48])\n","[DEBUG] Shape of positional_embedding: torch.Size([1, 226, 48])\n"]},{"ename":"RuntimeError","evalue":"The size of tensor a (37) must match the size of tensor b (226) at non-singleton dimension 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2465058945.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 43\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-898666820.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 108\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-449698712.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 124\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-449698712.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[DEBUG] Shape of positional_embedding: {self.positional_embedding.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (37) must match the size of tensor b (226) at non-singleton dimension 1"]}],"source":["\"\"\"\n","Args:\n","        image_size (int): Size of the input image (e.g., 224).\n","        patch_size (int): Size of each patch (e.g., 16).\n","        in_channels (int): Number of input channels (e.g., 1 for your task).\n","        num_classes (int): Number of output classes (e.g., 3 for your task).\n","        embed_dim (int): The main embedding dimension (e.g., 768 for ViT-Base).\n","        depth (int): Number of Transformer Encoder blocks (e.g., 12 for ViT-Base).\n","        num_heads (int): Number of attention heads (e.g., 12 for ViT-Base).\n","        mlp_ratio (float): Ratio to determine MLP hidden dimension (e.g., 4.0).\n","        dropout (float): Dropout probability.\n","\"\"\"\n","from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n","batch_size = 32\n","learning_rate = 1e-4\n","weight_decay = 0.05\n","num_epochs = 2000\n","warmup_epochs = 10\n","model = VisionTransformer(\n","        image_size=150, patch_size=10, in_channels=1, num_classes=3,\n","                 embed_dim=48, depth=2, num_heads=12, mlp_ratio=4.0, dropout=0.0\n","    )\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","\n","\n","#scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n","def warmup_lambda(current_epoch):\n","    if current_epoch \u003c warmup_epochs:\n","        return float(current_epoch) / float(max(1, warmup_epochs))\n","    return 1.0\n","warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n","main_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs, eta_min=1e-6)\n","scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])\n","\n","\n","print(\"Optimizer: Adam\")\n","print(f\"Learning Rate: {learning_rate}\")\n","\n","# Train Model\n","model, all_probs, all_labels = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":256,"status":"error","timestamp":1758069659340,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"f3b01b1e","outputId":"88849993-18b5-40fc-e207-5cd1dfb0a966"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimizer: Adam\n","Learning Rate: 0.0001\n","Training on device: cuda\n","===== Epoch 1/2000 =====\n"]},{"ename":"RuntimeError","evalue":"The size of tensor a (37) must match the size of tensor b (226) at non-singleton dimension 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2465058945.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 43\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-898666820.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 108\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-812423514.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 120\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-812423514.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (37) must match the size of tensor b (226) at non-singleton dimension 1"]}],"source":["\"\"\"\n","Args:\n","        image_size (int): Size of the input image (e.g., 224).\n","        patch_size (int): Size of each patch (e.g., 16).\n","        in_channels (int): Number of input channels (e.g., 1 for your task).\n","        num_classes (int): Number of output classes (e.g., 3 for your task).\n","        embed_dim (int): The main embedding dimension (e.g., 768 for ViT-Base).\n","        depth (int): Number of Transformer Encoder blocks (e.g., 12 for ViT-Base).\n","        num_heads (int): Number of attention heads (e.g., 12 for ViT-Base).\n","        mlp_ratio (float): Ratio to determine MLP hidden dimension (e.g., 4.0).\n","        dropout (float): Dropout probability.\n","\"\"\"\n","from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n","batch_size = 32\n","learning_rate = 1e-4\n","weight_decay = 0.05\n","num_epochs = 2000\n","warmup_epochs = 10\n","model = VisionTransformer(\n","        image_size=150, patch_size=10, in_channels=1, num_classes=3,\n","                 embed_dim=48, depth=2, num_heads=12, mlp_ratio=4.0, dropout=0.0\n","    )\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","\n","\n","#scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n","def warmup_lambda(current_epoch):\n","    if current_epoch \u003c warmup_epochs:\n","        return float(current_epoch) / float(max(1, warmup_epochs))\n","    return 1.0\n","warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n","main_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs, eta_min=1e-6)\n","scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])\n","\n","\n","print(\"Optimizer: Adam\")\n","print(f\"Learning Rate: {learning_rate}\")\n","\n","# Train Model\n","model, all_probs, all_labels = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"elapsed":306,"status":"error","timestamp":1758069689648,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"473904b0","outputId":"92073c35-bd25-468d-8d6f-f0f3ef20e051"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimizer: Adam\n","Learning Rate: 0.0001\n","Training on device: cuda\n","===== Epoch 1/2000 =====\n","[DEBUG] Shape of x before adding positional embedding: torch.Size([32, 37, 48])\n","[DEBUG] Shape of positional_embedding: torch.Size([1, 226, 48])\n"]},{"ename":"RuntimeError","evalue":"The size of tensor a (37) must match the size of tensor b (226) at non-singleton dimension 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2465058945.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 43\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-898666820.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 108\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-449698712.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 124\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-449698712.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[DEBUG] Shape of positional_embedding: {self.positional_embedding.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (37) must match the size of tensor b (226) at non-singleton dimension 1"]}],"source":["\"\"\"\n","Args:\n","        image_size (int): Size of the input image (e.g., 224).\n","        patch_size (int): Size of each patch (e.g., 16).\n","        in_channels (int): Number of input channels (e.g., 1 for your task).\n","        num_classes (int): Number of output classes (e.g., 3 for your task).\n","        embed_dim (int): The main embedding dimension (e.g., 768 for ViT-Base).\n","        depth (int): Number of Transformer Encoder blocks (e.g., 12 for ViT-Base).\n","        num_heads (int): Number of attention heads (e.g., 12 for ViT-Base).\n","        mlp_ratio (float): Ratio to determine MLP hidden dimension (e.g., 4.0).\n","        dropout (float): Dropout probability.\n","\"\"\"\n","from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n","batch_size = 32\n","learning_rate = 1e-4\n","weight_decay = 0.05\n","num_epochs = 2000\n","warmup_epochs = 10\n","model = VisionTransformer(\n","        image_size=150, patch_size=10, in_channels=1, num_classes=3,\n","                 embed_dim=48, depth=2, num_heads=12, mlp_ratio=4.0, dropout=0.0\n","    )\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","\n","\n","#scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n","def warmup_lambda(current_epoch):\n","    if current_epoch \u003c warmup_epochs:\n","        return float(current_epoch) / float(max(1, warmup_epochs))\n","    return 1.0\n","warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n","main_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs, eta_min=1e-6)\n","scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])\n","\n","\n","print(\"Optimizer: Adam\")\n","print(f\"Learning Rate: {learning_rate}\")\n","\n","# Train Model\n","model, all_probs, all_labels = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1758069570139,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"Q3rnNlt1_txE"},"outputs":[],"source":["import torch\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, roc_curve, auc\n","from sklearn.preprocessing import label_binarize\n","import copy\n","import matplotlib.pyplot as plt\n","from itertools import cycle\n","\n","def plot_roc_curve(all_labels, all_probs, class_names):\n","    \"\"\"\n","    Plots the ROC curve for each class and the micro/macro averages.\n","    \"\"\"\n","    # Binarize the labels for multi-class ROC analysis\n","    all_labels_bin = label_binarize(all_labels, classes=range(len(class_names)))\n","    all_probs = np.array(all_probs)\n","\n","    # Compute ROC curve and ROC area for each class\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    for i in range(len(class_names)):\n","        fpr[i], tpr[i], _ = roc_curve(all_labels_bin[:, i], all_probs[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","    # Compute micro-average ROC curve and ROC area\n","    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels_bin.ravel(), all_probs.ravel())\n","    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","\n","    # Compute macro-average ROC curve and ROC area\n","    # First aggregate all false positive rates\n","    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(class_names))]))\n","    # Then interpolate all ROC curves at these points\n","    mean_tpr = np.zeros_like(all_fpr)\n","    for i in range(len(class_names)):\n","        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n","    # Finally average it and compute AUC\n","    mean_tpr /= len(class_names)\n","    fpr[\"macro\"] = all_fpr\n","    tpr[\"macro\"] = mean_tpr\n","    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n","\n","    # Plot all ROC curves\n","    plt.figure(figsize=(10, 8))\n","\n","    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n","             label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n","             color='deeppink', linestyle=':', linewidth=4)\n","\n","    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n","             label=f'macro-average ROC curve (area = {roc_auc[\"macro\"]:.2f})',\n","             color='navy', linestyle=':', linewidth=4)\n","\n","    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n","    for i, color in zip(range(len(class_names)), colors):\n","        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n","                 label=f'ROC curve of class {class_names[i]} (area = {roc_auc[i]:.2f})')\n","\n","    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Multi-class Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend(loc=\"lower right\")\n","    plt.savefig(\"/content/drive/MyDrive/Model_III_dataset/roc_curve_heal_swin_pinn.png\")\n","    print(\"\\nROC curve plot saved as roc_curve.png\")\n","\n","\"\"\"Training and Evaluation with Early Stopping\"\"\"\n","def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, patience=2000):\n","    \"\"\"\n","    Trains the model with early stopping based on validation ROC AUC score.\n","\n","    Args:\n","        model (torch.nn.Module): The neural network model to train.\n","        train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n","        val_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n","        criterion: The loss function.\n","        optimizer: The optimization algorithm.\n","        scheduler: The learning rate scheduler.\n","        num_epochs (int): The maximum number of epochs to train for.\n","        patience (int): Number of epochs to wait for improvement before stopping.\n","    \"\"\"\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Training on device: {device}\")\n","\n","    model.to(device)\n","\n","    best_roc_auc = 0.0\n","    epochs_no_improve = 0\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_probs = []\n","    best_labels = []\n","\n","    class_names = ['axion', 'cdm', 'no_sub']\n","\n","    for epoch in range(num_epochs):\n","        print(f\"===== Epoch {epoch+1}/{num_epochs} =====\")\n","\n","        # --- Training Phase ---\n","        model.train()\n","        train_loss = 0.0\n","        train_correct = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            train_loss += loss.item() * images.size(0)\n","            _, predicted = torch.max(outputs.data, 1)\n","            train_correct += (predicted == labels).sum().item()\n","\n","        # --- Validation Phase ---\n","        model.eval()\n","        val_loss = 0.0\n","        val_correct = 0\n","        all_probs = []\n","        all_labels = []\n","\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * images.size(0)\n","                _, predicted = torch.max(outputs.data, 1)\n","                val_correct += (predicted == labels).sum().item()\n","\n","                probs = torch.softmax(outputs, dim=1)\n","                all_probs.extend(probs.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        # --- Calculate Metrics ---\n","        train_loss = train_loss / len(train_loader.dataset)\n","        val_loss = val_loss / len(val_loader.dataset)\n","        train_accuracy = train_correct / len(train_loader.dataset)\n","        val_accuracy = val_correct / len(val_loader.dataset)\n","\n","        # Calculate multi-class ROC AUC score\n","        all_labels_np = np.array(all_labels)\n","        all_probs_np = np.array(all_probs)\n","        try:\n","            val_roc_auc = roc_auc_score(all_labels_np, all_probs_np, multi_class='ovr', average='macro')\n","        except ValueError as e:\n","            print(f\"Could not calculate ROC AUC: {e}\")\n","            val_roc_auc = 0.0\n","\n","        # Epoch-level summary\n","        #print(f'\\n[SUMMARY] Epoch {epoch+1}/{num_epochs}:')\n","        #print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n","        #print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val ROC AUC: {val_roc_auc:.4f}')\n","\n","        if val_roc_auc \u003e best_roc_auc:\n","            best_roc_auc = val_roc_auc\n","            epochs_no_improve = 0\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            best_probs = all_probs\n","            best_labels = all_labels\n","            #torch.save(model.state_dict(), '/content/drive/MyDrive/Model_III_dataset/lens_classifier_model_vision_transformer.pth')\n","            print(f\"New best model saved with Val ROC AUC: {best_roc_auc:.4f}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n","        else:\n","            epochs_no_improve += 1\n","            print(f\"No improvement in Val ROC AUC for {epochs_no_improve} epoch(s). Best is {best_roc_auc:.4f}. Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n","\n","        if epochs_no_improve \u003e= patience:\n","            print(f\"\\nEarly stopping triggered after {patience} epochs without improvement.\")\n","            model.load_state_dict(best_model_wts)\n","            break\n","\n","    print(\"\\nTraining Complete!\")\n","    model.load_state_dict(best_model_wts)\n","\n","    # After the training loop, plot the ROC curve for the best model\n","    if best_probs and best_labels:\n","        plot_roc_curve(best_labels, best_probs, class_names)\n","\n","    return model, best_probs, best_labels"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1758069570223,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"9QBIRCLEkikD"},"outputs":[],"source":["#torch.save(model.state_dict(), '/content/drive/MyDrive/Model_III_dataset/model_weights.pth')"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":1190,"status":"error","timestamp":1758069571415,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"-ZJepKUw_txF","outputId":"1f349792-0c86-4593-ca07-9f55c0304cc1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimizer: Adam\n","Learning Rate: 0.0001\n","Training on device: cuda\n","===== Epoch 1/2000 =====\n"]},{"ename":"RuntimeError","evalue":"The size of tensor a (37) must match the size of tensor b (226) at non-singleton dimension 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2465058945.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 43\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-898666820.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 108\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4104270292.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 118\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-\u003e 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4104270292.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (37) must match the size of tensor b (226) at non-singleton dimension 1"]}],"source":["\"\"\"\n","Args:\n","        image_size (int): Size of the input image (e.g., 224).\n","        patch_size (int): Size of each patch (e.g., 16).\n","        in_channels (int): Number of input channels (e.g., 1 for your task).\n","        num_classes (int): Number of output classes (e.g., 3 for your task).\n","        embed_dim (int): The main embedding dimension (e.g., 768 for ViT-Base).\n","        depth (int): Number of Transformer Encoder blocks (e.g., 12 for ViT-Base).\n","        num_heads (int): Number of attention heads (e.g., 12 for ViT-Base).\n","        mlp_ratio (float): Ratio to determine MLP hidden dimension (e.g., 4.0).\n","        dropout (float): Dropout probability.\n","\"\"\"\n","from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n","batch_size = 32\n","learning_rate = 1e-4\n","weight_decay = 0.05\n","num_epochs = 2000\n","warmup_epochs = 10\n","model = VisionTransformer(\n","        image_size=150, patch_size=10, in_channels=1, num_classes=3,\n","                 embed_dim=48, depth=2, num_heads=12, mlp_ratio=4.0, dropout=0.0\n","    )\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","\n","\n","#scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n","def warmup_lambda(current_epoch):\n","    if current_epoch \u003c warmup_epochs:\n","        return float(current_epoch) / float(max(1, warmup_epochs))\n","    return 1.0\n","warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n","main_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs, eta_min=1e-6)\n","scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])\n","\n","\n","print(\"Optimizer: Adam\")\n","print(f\"Learning Rate: {learning_rate}\")\n","\n","# Train Model\n","model, all_probs, all_labels = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":70853,"status":"aborted","timestamp":1758069571412,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"G_-lEQtu_txF"},"outputs":[],"source":["\"\"\" ROC Curve Plotting Function\"\"\"\n","def plot_roc_curve(all_preds, all_labels):\n","    print(\"Generating ROC Curve\")\n","\n","    # Convert predictions and labels to numpy arrays\n","    all_preds = np.array(all_preds)\n","    all_labels = np.array(all_labels)\n","\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    n_classes = 3\n","\n","    for i in range(n_classes):\n","        fpr[i], tpr[i], _ = roc_curve((all_labels == i).astype(int), all_preds[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","        print(f\"Class {i} ROC AUC: {roc_auc[i]:.4f}\")\n","\n","    # Plot ROC curves\n","    plt.figure(figsize=(10, 8))\n","    colors = ['blue', 'red', 'green']\n","    class_names = ['Axion', 'CDM', 'No Substructure']\n","\n","    for i, color in zip(range(n_classes), colors):\n","        plt.plot(fpr[i], tpr[i], color=color,\n","                 label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n","\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend(loc=\"lower right\")\n","    plt.savefig('/content/drive/MyDrive/Model_III_dataset/roc_curve_vit.png')\n","    plt.close()\n","\n","    print(\"ROC Curve saved as roc_curve.png\")\n","\n","\n","plot_roc_curve(all_probs, all_labels)\n","\n","print(\"Training and Evaluation Complete!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":70976,"status":"aborted","timestamp":1758069571536,"user":{"displayName":"Dhruv Srivastava","userId":"05383083715668215672"},"user_tz":300},"id":"MZ59pFZa_txG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}