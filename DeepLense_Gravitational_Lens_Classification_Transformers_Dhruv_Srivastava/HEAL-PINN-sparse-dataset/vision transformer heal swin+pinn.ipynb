{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7Ekxmo1XV5I"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "executionInfo": {
     "elapsed": 121997,
     "status": "error",
     "timestamp": 1758136378041,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "_mFRKD8F__hl",
    "outputId": "5173fceb-5ee1-482e-e9b0-2ea2054f575b"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 409,
     "status": "aborted",
     "timestamp": 1758136378439,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "vRM2kxST8KJZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"Import dependencies\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 122615,
     "status": "aborted",
     "timestamp": 1758136378443,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "9Dm6SC1o_txB"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyDatasetViT(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.class_names = ['axion', 'cdm', 'no_sub']\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Loading dataset from: {data_dir}\")\n",
    "        print(f\"Looking for classes: {self.class_names}\")\n",
    "\n",
    "        for idx, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            print(f\"--- Processing class: {class_name} ---\")\n",
    "\n",
    "            if not os.path.exists(class_dir):\n",
    "                print(f\"[ERROR] Directory not found: {class_dir}\")\n",
    "                continue\n",
    "\n",
    "            files = os.listdir(class_dir)\n",
    "\n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.npy'):\n",
    "                    file_path = os.path.join(class_dir, file_name)\n",
    "                    loaded_data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "                    if class_name == 'axion':\n",
    "                        image = loaded_data[0]\n",
    "                    else:\n",
    "                        image = loaded_data\n",
    "\n",
    "                    # [DEBUG] Print the shape of the raw numpy array\n",
    "                    print(f\"  [DEBUG] Loaded '{file_name}'. Raw numpy shape: {image.shape}\")\n",
    "\n",
    "                    # Ensure the image is a 2D array (H, W) before adding channel dimension.\n",
    "                    if image.ndim != 2:\n",
    "                        image = np.squeeze(image)\n",
    "\n",
    "                    # Convert to a float tensor and add a channel dimension -> [1, H, W]\n",
    "                    image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                    # [DEBUG] Print the shape of the final tensor being stored in the dataset\n",
    "                    print(f\"  [DEBUG] Storing tensor with final shape: {image_tensor.shape}\\n\")\n",
    "\n",
    "                    self.data.append(image_tensor)\n",
    "                    self.labels.append(idx)\n",
    "\n",
    "        print(\"\\n--- Dataset Loading Complete ---\")\n",
    "        print(f\"Total images loaded: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This method is called by the DataLoader to get one item from the dataset.\n",
    "        The debug prints here are CRITICAL for finding the error.\n",
    "        \"\"\"\n",
    "        #print(f\"--- Getting item index: {idx} ---\")\n",
    "\n",
    "        # Retrieve the pre-loaded tensor and its label\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # [DEBUG] Print shape BEFORE the transform is applied\n",
    "        #print(f\"  [DEBUG] Shape of tensor BEFORE transform: {image.shape}\")\n",
    "\n",
    "        # Apply transformations (e.g., resizing) if they are provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            # [DEBUG] Print shape AFTER the transform is applied\n",
    "            #print(f\"  [DEBUG] Shape of tensor AFTER transform: {image.shape}\")\n",
    "        else:\n",
    "            #print(\"  [DEBUG] No transform was applied.\")\n",
    "            pass\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 122608,
     "status": "aborted",
     "timestamp": 1758136378446,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "VJEgowwT_txC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import the transforms module\n",
    "from torchvision import transforms\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Data Directories\n",
    "train_dir = '/content/drive/MyDrive/Model_V/Model_V'\n",
    "#val_dir = '../dataset/dataset/val'\n",
    "\n",
    "print(f\"Training Directory: {train_dir}\")\n",
    "#print(f\"Validation Directory: {val_dir}\")\n",
    "\n",
    "vit_transforms = transforms.Compose([\n",
    "    # transforms.ToTensor(), # Removed ToTensor()\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90), # Can be any angle range\n",
    "])\n",
    "\n",
    "# Create Datasets and Dataloaders\n",
    "#train_dataset = MyDataset(train_dir)\n",
    "#val_dataset = MyDataset(val_dir)\n",
    "#dataset = MyDatasetViT(train_dir, vit_transforms)\n",
    "#train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.75, 0.15, 0.1])\n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "#print(f\"Batch Size: {batch_size}\")\n",
    "#print(f\"Number of Training Batches: {len(train_loader)}\")\n",
    "#print(f\"Number of Validation Batches: {len(val_loader)}\")\n",
    "\n",
    "#Save the dataloader so that we don't have to bear with this pain again\n",
    "#torch.save(train_loader, '/content/drive/MyDrive/Model_III_dataset/train_loader.pth')\n",
    "#torch.save(val_loader, '/content/drive/MyDrive/Model_III_dataset/val_loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 122610,
     "status": "aborted",
     "timestamp": 1758136378449,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "gvqsjdgzKmOL"
   },
   "outputs": [],
   "source": [
    "#import data loaders from file\n",
    "train_loader = torch.load('/content/drive/MyDrive/train_loader.pth', weights_only=False)\n",
    "val_loader = torch.load('/content/drive/MyDrive/val_loader.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 122600,
     "status": "aborted",
     "timestamp": 1758136378452,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "y6Q9I1Hw_txD"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This implementation of the SWIN Transformer was adapted from the SWIN-UNet at\n",
    "https://github.com/HuCaoFighting/Swin-Unet/blob/main/networks/swin_transformer_unet_skip_expand_decoder_sys.py\n",
    "and modified for image classification tasks on specific input dimensions.\n",
    "\n",
    "The RelativisticPhysicalInformedEncoder has been added to the HEAL-Swin model.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Literal, Union, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.layers import DropPath, trunc_normal_\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" Multi-Layer Perceptron \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Partitions the input tensor into non-overlapping windows.\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (tuple[int]): Window size (height, width).\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Reverses the window partitioning.\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (tuple[int]): Window size (height, width).\n",
    "        H (int): Height of image.\n",
    "        W (int): Width of image.\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size[0] / window_size[1]))\n",
    "    x = windows.view(\n",
    "        B, H // window_size[0], W // window_size[1], window_size[0], window_size[1], -1\n",
    "    )\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both shifted and non-shifted windows.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        window_size,\n",
    "        num_heads,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "\n",
    "        # Define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
    "        )\n",
    "\n",
    "        # Get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.view(-1)\n",
    "        ].view(\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            -1,\n",
    "        )\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class TransformerLSABlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer block with LayerNorm, Multi-Head Self-Attention, and a Feed-Forward Network.\n",
    "    This is a standard component for transformer architectures.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, num_patches, num_hidden_neurons, activation_function, device, dropout):\n",
    "        super(TransformerLSABlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, num_hidden_neurons),\n",
    "            activation_function(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(num_hidden_neurons, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-Head Self-Attention\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_output, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + self.dropout(attn_output)\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        x_norm = self.norm2(x)\n",
    "        mlp_output = self.mlp(x_norm)\n",
    "        x = x + self.dropout(mlp_output)\n",
    "        return x\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        num_heads,\n",
    "        window_size=(5, 5),\n",
    "        shift_size=(0, 0),\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        if min(self.input_resolution) <= min(self.window_size):\n",
    "            self.shift_size = (0, 0)\n",
    "            self.window_size = self.input_resolution\n",
    "\n",
    "        assert 0 <= self.shift_size[0] < self.window_size[0], \"shift_size must be less than window_size\"\n",
    "        assert 0 <= self.shift_size[1] < self.window_size[1], \"shift_size must be less than window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=self.window_size,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "        if self.shift_size[0] > 0 or self.shift_size[1] > 0:\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))\n",
    "            h_slices = (\n",
    "                slice(0, -self.window_size[0]),\n",
    "                slice(-self.window_size[0], -self.shift_size[0]),\n",
    "                slice(-self.shift_size[0], None),\n",
    "            )\n",
    "            w_slices = (\n",
    "                slice(0, -self.window_size[1]),\n",
    "                slice(-self.window_size[1], -self.shift_size[1]),\n",
    "                slice(-self.shift_size[1], None),\n",
    "            )\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)\n",
    "            mask_windows = mask_windows.view(-1, self.window_size[0] * self.window_size[1])\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n",
    "                attn_mask == 0, float(0.0)\n",
    "            )\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        if self.shift_size[0] > 0 or self.shift_size[1] > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size[0], -self.shift_size[1]), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size[0] * self.window_size[1], C)\n",
    "\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
    "\n",
    "        attn_windows = attn_windows.view(-1, self.window_size[0], self.window_size[1], C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "\n",
    "        if self.shift_size[0] > 0 or self.shift_size[1] > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size[0], self.shift_size[1]), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage. \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,\n",
    "                    input_resolution=input_resolution,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=window_size,\n",
    "                    shift_size=(0, 0) if (i % 2 == 0) else (window_size[0] // 2, window_size[1] // 2),\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=(150, 150), patch_size=(10, 10), in_chans=1, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.num_patches = self.patches_resolution[0] * self.patches_resolution[1]\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class RelativisticPhysicalInformedEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module to perform inverse gravitational lensing using the Singular Isothermal Sphere (SIS) model.\n",
    "\n",
    "    Attributes:\n",
    "        pixel_scale (float): The scale of each pixel in the image, often in arcseconds per pixel.\n",
    "        profile_size (int): The size of the image profile.\n",
    "        half_profile_size (int): Half of the profile size.\n",
    "        num_patches (int): Number of patches.\n",
    "        embed_dim (int): Dimension of the embedding.\n",
    "        num_heads (int): Number of heads in the transformer.\n",
    "        num_hidden_neurons (int): Number of hidden neurons.\n",
    "        eps (float): Float number used to avoid division by zero.\n",
    "        transformer_activation_function (nn.Module): Activation function used in the transformer.\n",
    "        num_transformer_blocks (int): Number of transformer blocks.\n",
    "        device (torch.device): Device to which tensors will be moved.\n",
    "        transformer (nn.ModuleList): List of transformer blocks.\n",
    "        num_neurons_flatten (int): Number of neurons in the Flatten layer.\n",
    "        grid_x (Tensor): Grid of x coordinates.\n",
    "        grid_y (Tensor): Grid of y coordinates.\n",
    "        flat_grid_x (Tensor): Flattened grid of x coordinates.\n",
    "        flat_grid_y (Tensor): Flattened grid of y coordinates.\n",
    "    \"\"\"\n",
    "    def __init__( self,\n",
    "                  image_size: int,\n",
    "                  patch_size: int ,\n",
    "                  embed_dim: int,\n",
    "                  num_patches: int,\n",
    "                  num_heads: int,\n",
    "                  num_hidden_neurons: int,\n",
    "                  transformer_activation_function: nn.Module,\n",
    "                  num_transformer_blocks: int,\n",
    "                  device: torch.device,\n",
    "                  dropout: float = 0.1,\n",
    "                  pixel_scale:float =0.101,\n",
    "                  k_min: float = 0.8,\n",
    "                  k_max: float = 1.2,\n",
    "                  eps: float = 1e-8\n",
    "                  ):\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the module.\n",
    "\n",
    "        Args:\n",
    "            image_size (int): The size of the image.\n",
    "            patch_size (int): The size of each patch.\n",
    "            embed_dim (int): The embedding dimension.\n",
    "            num_patches (int): The number of patches.\n",
    "            num_heads (int): The number of heads in the transformer.\n",
    "            num_hidden_neurons (int): The number of hidden neurons.\n",
    "            transformer_activation_function (nn.Module): The activation function used in the transformer.\n",
    "            num_transformer_blocks (int): The number of transformer blocks.\n",
    "            device (torch.device): The device to which tensors will be moved.\n",
    "            dropout (float, optional): The dropout rate. Defaults to 0.1.\n",
    "            pixel_scale (float, optional): The scale of each pixel in the image, often in arcseconds per pixel. Defaults to 0.101.\n",
    "            k_min (float, optional): Minimum value of the potential correction parameter.  Defaults to 0.8.\n",
    "            k_max (float, optional): Maximum value of the potencial correction parameter.  Defaults to 1.2.\n",
    "            eps (float): Float number used to avoid division by zero. Defaluts to 1e-8.\n",
    "        \"\"\"\n",
    "\n",
    "        super(RelativisticPhysicalInformedEncoder, self).__init__()\n",
    "\n",
    "        # Initialize variables\n",
    "        self.pixel_scale = pixel_scale\n",
    "        self.profile_size = image_size\n",
    "        self.half_profile_size = self.profile_size // 2\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        self.transformer_activation_function = transformer_activation_function\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.device = device\n",
    "        self.k_min = k_min\n",
    "        self.k_max = k_max\n",
    "        self.eps = eps\n",
    "\n",
    "        # Create an empty list for the transformer blocks\n",
    "        self.transformer = nn.ModuleList()\n",
    "\n",
    "        # Calculate the number of neurons for the Flatten layer\n",
    "        self.num_neurons_flatten = (self.num_patches)*embed_dim\n",
    "\n",
    "        # Create a for loop that iterates over the number of transformer blocks\n",
    "        for _ in range(num_transformer_blocks):\n",
    "          # Add a TransformerLSABlock to the transformer list\n",
    "          self.transformer.append(\n",
    "\n",
    "              TransformerLSABlock(embed_dim,\n",
    "                                  num_heads,\n",
    "                                  self.num_patches,\n",
    "                                  num_hidden_neurons,\n",
    "                                  transformer_activation_function,\n",
    "                                  device,\n",
    "                                  dropout)\n",
    "          )\n",
    "\n",
    "        # Create a compressor (FeedFoward) compress the size of num_neurons_flatten\n",
    "        self.transformer.append(nn.Flatten())\n",
    "        self.transformer.append(nn.Linear(self.num_neurons_flatten,self.profile_size*self.profile_size))\n",
    "        self.transformer.append(nn.Sigmoid())\n",
    "\n",
    "        # Create a grid for image coordinates\n",
    "        x_coordinates = torch.linspace(-self.half_profile_size, self.half_profile_size-1, self.profile_size) * self.pixel_scale\n",
    "        y_coordinates = torch.linspace(-self.half_profile_size, self.half_profile_size-1, self.profile_size) * self.pixel_scale\n",
    "        self.grid_x, self.grid_y = torch.meshgrid(x_coordinates, y_coordinates, indexing='ij')\n",
    "        self.flat_grid_x = self.grid_x.flatten().to(self.device)\n",
    "        self.flat_grid_y = self.grid_y.flatten().to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, input_images: Tensor, patches: Tensor)->Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        Forward pass through the module.\n",
    "\n",
    "        Args:\n",
    "            input_images (torch.Tensor): The input images.\n",
    "            patches (torch.Tensor): The patches extracted from the images.\n",
    "\n",
    "        Returns:\n",
    "            output_images (torch.Tensor): The output images after inverse gravitational lensing.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the batch size from the input images\n",
    "        batch_size = input_images.shape[0]\n",
    "\n",
    "        # Generate k using the sequential model\n",
    "        k_sigmoid = patches\n",
    "        for layer in self.transformer:\n",
    "            k_sigmoid = layer(k_sigmoid)\n",
    "\n",
    "        # Reshape k_sigmoid to have shape [batch_size, profile_size, profile_size]\n",
    "        k_sigmoid = k_sigmoid.view(-1, self.profile_size, self.profile_size)\n",
    "\n",
    "        # Flatten k_sigmoid to match the shape of non_zero_x and non_zero_radius\n",
    "        k_sigmoid_flat = k_sigmoid.view(-1, self.profile_size*self.profile_size)\n",
    "\n",
    "        # Bias and Scalling\n",
    "        k_sigmoid_flat = self.k_min + (self.k_max-self.k_min)*k_sigmoid_flat\n",
    "\n",
    "         # Flatten the input images for easier indexing\n",
    "        flat_input_images = input_images.view(batch_size, -1)\n",
    "\n",
    "        # Create a mask for non-zero coordinates in the grid\n",
    "        non_zero_mask = (self.flat_grid_x != 0) | (self.flat_grid_y != 0)\n",
    "\n",
    "        # Select only the non-zero indices to match with non_zero_x and non_zero_radius\n",
    "        k_sigmoid_non_zero = k_sigmoid_flat[:, non_zero_mask]\n",
    "\n",
    "        # Get the shape of k_sigmoid_non_zero\n",
    "        shape_k_sigmoid_non_zero = k_sigmoid_non_zero.shape\n",
    "\n",
    "        # Reshape k to have a batch dimension compatible for broadcasting\n",
    "        k = k_sigmoid_non_zero.view(shape_k_sigmoid_non_zero[0], 1, 1, shape_k_sigmoid_non_zero[1])\n",
    "\n",
    "        # Apply the mask to get non-zero coordinates\n",
    "        non_zero_x = self.flat_grid_x[non_zero_mask]\n",
    "        non_zero_y = self.flat_grid_y[non_zero_mask]\n",
    "\n",
    "        # Calculate the radius for non-zero coordinates\n",
    "        non_zero_radius = torch.sqrt(non_zero_x ** 2 + non_zero_y ** 2)\n",
    "\n",
    "        # Expand dimensions for broadcasting\n",
    "        non_zero_radius = non_zero_radius[None, None, None, :]\n",
    "\n",
    "        # Compute shifted coordinates based on the Gravitational Lens Equation to SIS model\n",
    "        shifted_x = (non_zero_x[None, None, None, :] - k * non_zero_x[None, None, None, :] / (non_zero_radius + self.eps) )\n",
    "        shifted_y = (non_zero_y[None, None, None, :] - k * non_zero_y[None, None, None, :] / (non_zero_radius + self.eps) )\n",
    "\n",
    "        # Convert shifted coordinates to indices in the image grid\n",
    "        shifted_x_idx = torch.round(shifted_x / self.pixel_scale + self.half_profile_size).long()\n",
    "        shifted_y_idx = torch.round(shifted_y / self.pixel_scale + self.half_profile_size).long()\n",
    "\n",
    "        # Clamp indices to be within the valid range\n",
    "        shifted_x_idx = torch.clamp(shifted_x_idx, 0, self.profile_size - 1)\n",
    "        shifted_y_idx = torch.clamp(shifted_y_idx, 0, self.profile_size - 1)\n",
    "\n",
    "\n",
    "        # Initialize the output image tensor and flatten it\n",
    "        output_images = torch.zeros(batch_size, self.profile_size, self.profile_size).to(self.device)\n",
    "        flat_output_images = output_images.view(batch_size, -1)\n",
    "\n",
    "        # Calculate 1D indices from shifted_x_idx and shifted_y_idx\n",
    "        one_d_indices = shifted_x_idx * self.profile_size + shifted_y_idx\n",
    "\n",
    "        # Flatten the input images for easier indexing\n",
    "        flat_input_images = input_images.view(batch_size, -1)\n",
    "\n",
    "        # Get the current values at the shifted positions in the flat output images\n",
    "        output_values_at_shifted_positions = flat_output_images.gather(1, one_d_indices.view(batch_size, -1))\n",
    "\n",
    "        # Get the corresponding values from the original positions in the flat input images\n",
    "        input_values_at_original_positions = flat_input_images[:, non_zero_mask]\n",
    "\n",
    "        # Update the output image based on the algorithm\n",
    "        updated_values = torch.where(output_values_at_shifted_positions == 0,\n",
    "                                    input_values_at_original_positions,\n",
    "                                    (output_values_at_shifted_positions + input_values_at_original_positions) / 2)\n",
    "\n",
    "        # Assign the updated values back to the flat output images\n",
    "        flat_output_images.scatter_(1, one_d_indices.view(batch_size, -1), updated_values)\n",
    "\n",
    "        # Reshape the flat output images back to their original shape\n",
    "        output_images = flat_output_images.view(batch_size, self.profile_size, self.profile_size)\n",
    "\n",
    "\n",
    "        # Normalize the output images\n",
    "        max_values, _ = output_images.max(dim=1, keepdim=True)\n",
    "        max_values, _ = max_values.max(dim=2, keepdim=True)\n",
    "        output_images = output_images / (max_values + self.eps)\n",
    "\n",
    "        return output_images.unsqueeze(1) # Add channel dimension back\n",
    "\n",
    "\n",
    "class HEAL_Swin(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer with a Relativistic Physical Informed Encoder.\n",
    "    This model first applies a physics-informed transformation to the input image\n",
    "    before processing it with the Swin Transformer for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=(150, 150), patch_size=(10, 10), in_chans=1, num_classes=3,\n",
    "                 embed_dim=96, depths=[2, 2, 6], num_heads=[3, 6, 12],\n",
    "                 window_size=(5, 5), mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 # Relativistic Encoder Params\n",
    "                 relativistic_encoder_params=None,\n",
    "                 device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim)\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.device = device\n",
    "\n",
    "        # Split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # Relativistic Physical Informed Encoder\n",
    "        if relativistic_encoder_params is None:\n",
    "            relativistic_encoder_params = {\n",
    "                'image_size': img_size[0],\n",
    "                'patch_size': patch_size[0],\n",
    "                'embed_dim': embed_dim,\n",
    "                'num_patches': num_patches,\n",
    "                'num_heads': num_heads[0],\n",
    "                'num_hidden_neurons': int(embed_dim * mlp_ratio),\n",
    "                'transformer_activation_function': nn.GELU,\n",
    "                'num_transformer_blocks': 2,\n",
    "                'device': self.device\n",
    "            }\n",
    "        self.relativistic_encoder = RelativisticPhysicalInformedEncoder(**relativistic_encoder_params)\n",
    "\n",
    "\n",
    "        # Absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        # Build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        layer = BasicLayer(dim=int(embed_dim),\n",
    "                           input_resolution=patches_resolution,\n",
    "                           depth=sum(depths),\n",
    "                           num_heads=num_heads[0],\n",
    "                           window_size=window_size,\n",
    "                           mlp_ratio=self.mlp_ratio,\n",
    "                           qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                           drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                           drop_path=dpr,\n",
    "                           norm_layer=norm_layer,\n",
    "                           downsample=None)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # First, get the patches\n",
    "        patches = self.patch_embed(x)\n",
    "\n",
    "        # Pass original image and patches to the relativistic encoder\n",
    "        x_encoded = self.relativistic_encoder(x, patches)\n",
    "\n",
    "        # Now, embed the encoded image\n",
    "        x = self.patch_embed(x_encoded)\n",
    "\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.avgpool(x.transpose(1, 2))\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 122600,
     "status": "aborted",
     "timestamp": 1758136378454,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "hivvAHcI41JJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#!pip install torch_xla[tpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 122604,
     "status": "aborted",
     "timestamp": 1758136378459,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "GtiRNfT34-6a"
   },
   "outputs": [],
   "source": [
    "#import torch_xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 122607,
     "status": "aborted",
     "timestamp": 1758136378463,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "Q3rnNlt1_txE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "def plot_roc_curve(all_labels, all_probs, class_names):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve for each class and the micro/macro averages.\n",
    "    \"\"\"\n",
    "    # Binarize the labels for multi-class ROC analysis\n",
    "    all_labels_bin = label_binarize(all_labels, classes=range(len(class_names)))\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(len(class_names)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(all_labels_bin[:, i], all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels_bin.ravel(), all_probs.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(class_names))]))\n",
    "    # Then interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(len(class_names)):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= len(class_names)\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label=f'macro-average ROC curve (area = {roc_auc[\"macro\"]:.2f})',\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(len(class_names)), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of class {class_names[i]} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-class Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(\"/content/drive/MyDrive/Model_III_dataset/roc_curve_heal_swin_pinn.png\")\n",
    "    print(\"\\nROC curve plot saved as roc_curve.png\")\n",
    "\n",
    "\"\"\"Training and Evaluation with Early Stopping\"\"\"\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, patience=2000):\n",
    "    \"\"\"\n",
    "    Trains the model with early stopping based on validation ROC AUC score.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to train.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n",
    "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n",
    "        criterion: The loss function.\n",
    "        optimizer: The optimization algorithm.\n",
    "        scheduler: The learning rate scheduler.\n",
    "        num_epochs (int): The maximum number of epochs to train for.\n",
    "        patience (int): Number of epochs to wait for improvement before stopping.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    best_roc_auc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_probs = []\n",
    "    best_labels = []\n",
    "\n",
    "    class_names = ['axion', 'cdm', 'no_sub']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"===== Epoch {epoch+1}/{num_epochs} =====\")\n",
    "\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # --- Calculate Metrics ---\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        # Calculate multi-class ROC AUC score\n",
    "        all_labels_np = np.array(all_labels)\n",
    "        all_probs_np = np.array(all_probs)\n",
    "        try:\n",
    "            val_roc_auc = roc_auc_score(all_labels_np, all_probs_np, multi_class='ovr', average='macro')\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not calculate ROC AUC: {e}\")\n",
    "            val_roc_auc = 0.0\n",
    "\n",
    "        # Epoch-level summary\n",
    "        #print(f'\\n[SUMMARY] Epoch {epoch+1}/{num_epochs}:')\n",
    "        #print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "        #print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val ROC AUC: {val_roc_auc:.4f}')\n",
    "\n",
    "        if val_roc_auc > best_roc_auc:\n",
    "            best_roc_auc = val_roc_auc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_probs = all_probs\n",
    "            best_labels = all_labels\n",
    "            #torch.save(model.state_dict(), '/content/drive/MyDrive/Model_III_dataset/lens_classifier_model_vision_transformer.pth')\n",
    "            print(f\"New best model saved with Val ROC AUC: {best_roc_auc:.4f}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement in Val ROC AUC for {epochs_no_improve} epoch(s). Best is {best_roc_auc:.4f}. Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {patience} epochs without improvement.\")\n",
    "            model.load_state_dict(best_model_wts)\n",
    "            break\n",
    "\n",
    "    print(\"\\nTraining Complete!\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # After the training loop, plot the ROC curve for the best model\n",
    "    if best_probs and best_labels:\n",
    "        plot_roc_curve(best_labels, best_probs, class_names)\n",
    "\n",
    "    return model, best_probs, best_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 122606,
     "status": "aborted",
     "timestamp": 1758136378466,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "9QBIRCLEkikD"
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), '/content/drive/MyDrive/Model_III_dataset/model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 122601,
     "status": "aborted",
     "timestamp": 1758136378470,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "-ZJepKUw_txF"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "        image_size (int): Size of the input image (e.g., 224).\n",
    "        patch_size (int): Size of each patch (e.g., 16).\n",
    "        in_channels (int): Number of input channels (e.g., 1 for your task).\n",
    "        num_classes (int): Number of output classes (e.g., 3 for your task).\n",
    "        embed_dim (int): The main embedding dimension (e.g., 768 for ViT-Base).\n",
    "        depth (int): Number of Transformer Encoder blocks (e.g., 12 for ViT-Base).\n",
    "        num_heads (int): Number of attention heads (e.g., 12 for ViT-Base).\n",
    "        mlp_ratio (float): Ratio to determine MLP hidden dimension (e.g., 4.0).\n",
    "        dropout (float): Dropout probability.\n",
    "\"\"\"\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.05\n",
    "num_epochs = 2000\n",
    "warmup_epochs = 10\n",
    "\"\"\"\n",
    "model = SwinTransformerForClassification(\n",
    "        img_size=(150, 150),\n",
    "        patch_size=(10, 10),\n",
    "        in_chans=1,\n",
    "        num_classes=3,      # Same number of classes as the ViT example\n",
    "        embed_dim=48,\n",
    "        depths=[2, 2, 2, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=(5, 5),\n",
    "        drop_path_rate=0.2,\n",
    "    )\"\"\"\n",
    "model = HEAL_Swin(img_size=(64, 64), patch_size=(8, 8), num_classes=3, window_size=(8, 8))\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.05)\n",
    "\n",
    "\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "def warmup_lambda(current_epoch):\n",
    "    if current_epoch < warmup_epochs:\n",
    "        return float(current_epoch) / float(max(1, warmup_epochs))\n",
    "    return 1.0\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "main_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs, eta_min=1e-6)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])\n",
    "\n",
    "\n",
    "print(\"Optimizer: Adam\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "\n",
    "# Train Model\n",
    "model, all_probs, all_labels = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_-lEQtu_txF"
   },
   "source": [
    "\"\"\" ROC Curve Plotting Function\"\"\"\n",
    "def plot_roc_curve(all_preds, all_labels):\n",
    "    print(\"Generating ROC Curve\")\n",
    "\n",
    "    # Convert predictions and labels to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    n_classes = 3\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve((all_labels == i).astype(int), all_preds[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        print(f\"Class {i} ROC AUC: {roc_auc[i]:.4f}\")\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    class_names = ['Axion', 'CDM', 'No Substructure']\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color,\n",
    "                 label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('/content/drive/MyDrive/Model_III_dataset/roc_curve_heal_swin_pinn.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"ROC Curve saved as roc_curve.png\")\n",
    "\n",
    "\n",
    "plot_roc_curve(all_probs, all_labels)\n",
    "\n",
    "print(\"Training and Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 122603,
     "status": "aborted",
     "timestamp": 1758136378474,
     "user": {
      "displayName": "Dhruv Srivastava",
      "userId": "05383083715668215672"
     },
     "user_tz": 300
    },
    "id": "MZ59pFZa_txG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1r40O3vAh8rRdhVOou48Z6yaV7mWYApFU",
     "timestamp": 1756334887300
    },
    {
     "file_id": "1ghkT6ZbFHFHhRUGGQefyNf9CpDdFjamf",
     "timestamp": 1756245539652
    },
    {
     "file_id": "1wEko_Ik-lhlsW1rYndRXwX-Mgx9jST28",
     "timestamp": 1756243482189
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
