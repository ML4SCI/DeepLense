{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Ekxmo1XV5I"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "import shutil\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG_PATH = Path(\"../configs/lensid.yaml\")\n",
        "\n",
        "with open(CONFIG_PATH, \"r\") as f:\n",
        "    cfg = yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #Google Drive mounting is required only when running in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "figures_dir = Path(cfg[\"outputs\"][\"figures_dir\"])\n",
        "models_dir = Path(cfg[\"outputs\"][\"models_dir\"])\n",
        "\n",
        "figures_dir.mkdir(parents=True, exist_ok=True)\n",
        "models_dir.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- NOTE:\n",
        "- Dataset artifacts (Model_III.tgz and preprocessed loaders)\n",
        "- are not included in the repository.\n",
        "- Users must place them manually at cfg[\"data\"][\"archive_path\"]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HwTHNYYAoBC"
      },
      "outputs": [],
      "source": [
        "archive_path = Path(cfg[\"data\"][\"archive_path\"])\n",
        "extract_dir = Path(cfg[\"data\"][\"extract_dir\"])\n",
        "\n",
        "if not archive_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Dataset archive not found at {archive_path}. \"\n",
        "        \"Please place Model_III.tgz in the data/ directory.\"\n",
        "    )\n",
        "\n",
        "if not extract_dir.exists():\n",
        "    shutil.unpack_archive(archive_path, extract_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRM2kxST8KJZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "GSoC 2025 Internship Application Task - 1\n",
        "Author: Dhruv Srivastava\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"Import dependencies\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet18\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dm6SC1o_txB"
      },
      "outputs": [],
      "source": [
        "\"\"\"Define Dataset Class for Vision Transformer with Debugging\"\"\"\n",
        "class MyDatasetViT(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.class_names = ['axion', 'cdm', 'no_sub']\n",
        "        self.transform = transform\n",
        "\n",
        "        print(f\"Loading dataset from: {data_dir}\")\n",
        "        print(f\"Looking for classes: {self.class_names}\")\n",
        "\n",
        "        for idx, class_name in enumerate(self.class_names):\n",
        "            class_dir = os.path.join(data_dir, class_name)\n",
        "            print(f\"--- Processing class: {class_name} ---\")\n",
        "\n",
        "            if not os.path.exists(class_dir):\n",
        "                print(f\"[ERROR] Directory not found: {class_dir}\")\n",
        "                continue\n",
        "\n",
        "            files = os.listdir(class_dir)\n",
        "\n",
        "            for file_name in files:\n",
        "                if file_name.endswith('.npy'):\n",
        "                    file_path = os.path.join(class_dir, file_name)\n",
        "                    loaded_data = np.load(file_path, allow_pickle=True)\n",
        "\n",
        "                    if class_name == 'axion':\n",
        "                        image = loaded_data[0]\n",
        "                    else:\n",
        "                        image = loaded_data\n",
        "\n",
        "                    # [DEBUG] Print the shape of the raw numpy array\n",
        "                    print(f\"  [DEBUG] Loaded '{file_name}'. Raw numpy shape: {image.shape}\")\n",
        "\n",
        "                    # Ensure the image is a 2D array (H, W) before adding channel dimension.\n",
        "                    if image.ndim != 2:\n",
        "                        image = np.squeeze(image)\n",
        "\n",
        "                    # Convert to a float tensor and add a channel dimension -> [1, H, W]\n",
        "                    image_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "                    # [DEBUG] Print the shape of the final tensor being stored in the dataset\n",
        "                    print(f\"  [DEBUG] Storing tensor with final shape: {image_tensor.shape}\\n\")\n",
        "\n",
        "                    self.data.append(image_tensor)\n",
        "                    self.labels.append(idx)\n",
        "\n",
        "        print(\"\\n--- Dataset Loading Complete ---\")\n",
        "        print(f\"Total images loaded: {len(self.data)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        This method is called by the DataLoader to get one item from the dataset.\n",
        "        The debug prints here are CRITICAL for finding the error.\n",
        "        \"\"\"\n",
        "        #print(f\"--- Getting item index: {idx} ---\")\n",
        "\n",
        "        # Retrieve the pre-loaded tensor and its label\n",
        "        image = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # [DEBUG] Print shape BEFORE the transform is applied\n",
        "        #print(f\"  [DEBUG] Shape of tensor BEFORE transform: {image.shape}\")\n",
        "\n",
        "        # Apply transformations (e.g., resizing) if they are provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            # [DEBUG] Print shape AFTER the transform is applied\n",
        "            #print(f\"  [DEBUG] Shape of tensor AFTER transform: {image.shape}\")\n",
        "        else:\n",
        "            #print(\"  [DEBUG] No transform was applied.\")\n",
        "            pass\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJEgowwT_txC",
        "outputId": "de7672d2-1191-4930-adc3-37c424d0ad7c"
      },
      "outputs": [],
      "source": [
        "# Import the transforms module\n",
        "from torchvision import transforms\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "# Data Directories\n",
        "train_dir = f'{cfg[\"data\"][\"dataset_root\"]}/train'\n",
        "#val_dir = '../dataset/dataset/val'\n",
        "\n",
        "print(f\"Training Directory: {train_dir}\")\n",
        "#print(f\"Validation Directory: {val_dir}\")\n",
        "\n",
        "vit_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64), antialias=True)\n",
        "])\n",
        "\n",
        "# Create Datasets and Dataloaders\n",
        "#train_dataset = MyDataset(train_dir)\n",
        "#val_dataset = MyDataset(val_dir)\n",
        "#dataset = MyDatasetViT(train_dir, vit_transforms)\n",
        "#train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.75, 0.15, 0.1])\n",
        "\n",
        "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "#val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "\n",
        "#print(f\"Batch Size: {batch_size}\")\n",
        "#print(f\"Number of Training Batches: {len(train_loader)}\")\n",
        "#print(f\"Number of Validation Batches: {len(val_loader)}\")\n",
        "\n",
        "#Save the dataloader so that we don't have to bear with this pain again\n",
        "#torch.save(train_loader, '/content/drive/MyDrive/Model_III_dataset/train_loader.pth')\n",
        "#torch.save(val_loader, '/content/drive/MyDrive/Model_III_dataset/val_loader.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvqsjdgzKmOL"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "dataset_root = Path(cfg[\"data\"][\"dataset_root\"])\n",
        "\n",
        "train_loader_path = dataset_root / \"train_loader.pth\"\n",
        "val_loader_path = dataset_root / \"val_loader.pth\"\n",
        "\n",
        "if not train_loader_path.exists() or not val_loader_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"Preprocessed DataLoader files not found.\\n\"\n",
        "        \"Expected:\\n\"\n",
        "        f\"  - {train_loader_path}\\n\"\n",
        "        f\"  - {val_loader_path}\\n\\n\"\n",
        "        \"Please run the dataset preprocessing step first \"\n",
        "        \"or obtain the preprocessed files.\"\n",
        "    )\n",
        "\n",
        "train_loader = torch.load(train_loader_path, weights_only=False)\n",
        "val_loader = torch.load(val_loader_path, weights_only=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Q9I1Hw_txD",
        "outputId": "3cbb9dc6-5dc1-4d81-cc79-3feb9bd3cf9f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import DropPath\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        self.projection = nn.Conv2d(\n",
        "            in_channels,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, C, H, W) -> (B, E, N_patches_sqrt, N_patches_sqrt)\n",
        "        x = self.projection(x)\n",
        "        # (B, E, N_patches_sqrt, N_patches_sqrt) -> (B, E, N)\n",
        "        x = x.flatten(2)\n",
        "        # (B, E, N) -> (B, N, E)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # --- FIX IS HERE ---\n",
        "        # Get the batch size from the input tensor x\n",
        "        batch_size = x.shape[0]\n",
        "        # Expand the CLS token to match the batch size\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "\n",
        "        # Prepend the CLS token to the patch embeddings\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.positional_embedding\n",
        "\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        # --- FIX IS HERE ---\n",
        "        # Unpack q, k, v from the first dimension\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1, drop_path_rate=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=embed_dim, hidden_features=mlp_hidden_dim, out_features=embed_dim, dropout=dropout)\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size=224, patch_size=16, in_channels=1, num_classes=3,\n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, dropout=0.1, drop_path_rate = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(\n",
        "                embed_dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout,\n",
        "                drop_path_rate = dpr[i]\n",
        "            ) for i in range(depth)])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        for block in self.encoder_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        cls_token_final = x[:, 0]\n",
        "        output = self.head(cls_token_final)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hivvAHcI41JJ",
        "outputId": "257c8cec-1eba-478e-ca04-286fede0f97c"
      },
      "outputs": [],
      "source": [
        "#!pip install torch_xla[tpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "GtiRNfT34-6a",
        "outputId": "c9533c3a-f67e-4fcd-fce4-a4af6e412ce4"
      },
      "outputs": [],
      "source": [
        "#import torch_xla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3rnNlt1_txE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from pathlib import Path\n",
        "import copy\n",
        "\n",
        "\"\"\"Training and Evaluation with Early Stopping\"\"\"\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, patience=10):\n",
        "    \"\"\"\n",
        "    Trains the model with early stopping based on validation ROC AUC score.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The neural network model to train.\n",
        "        train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n",
        "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n",
        "        criterion: The loss function.\n",
        "        optimizer: The optimization algorithm.\n",
        "        scheduler: The learning rate scheduler.\n",
        "        num_epochs (int): The maximum number of epochs to train for.\n",
        "        patience (int): Number of epochs to wait for improvement before stopping.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Training on device: {device}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    best_roc_auc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    class_names = ['axion', 'cdm', 'no_sub']\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
        "\n",
        "        # --- Training Phase ---\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        all_probs = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # --- Calculate Metrics ---\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        train_accuracy = train_correct / len(train_loader.dataset)\n",
        "        val_accuracy = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        # Calculate multi-class ROC AUC score\n",
        "        all_labels_np = np.array(all_labels)\n",
        "        all_probs_np = np.array(all_probs)\n",
        "        try:\n",
        "            val_roc_auc = roc_auc_score(all_labels_np, all_probs_np, multi_class='ovr', average='macro')\n",
        "        except ValueError as e:\n",
        "            print(f\"Could not calculate ROC AUC: {e}\")\n",
        "            val_roc_auc = 0.0\n",
        "\n",
        "        # Epoch-level summary\n",
        "        print(f'\\n[SUMMARY] Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val ROC AUC: {val_roc_auc:.4f}')\n",
        "\n",
        "        if val_roc_auc > best_roc_auc:\n",
        "            best_roc_auc = val_roc_auc\n",
        "            epochs_no_improve = 0\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(                #Using config-driven model output path instead of hard-coded filename\n",
        "                model.state_dict(),\n",
        "                models_dir / cfg[\"outputs\"][\"model_name\"]\n",
        "            )\n",
        "            print(f\"New best model saved with Val ROC AUC: {best_roc_auc:.4f}\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"No improvement in Val ROC AUC for {epochs_no_improve} epoch(s). Best is {best_roc_auc:.4f}.\")\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"\\nEarly stopping triggered after {patience} epochs without improvement.\")\n",
        "            model.load_state_dict(best_model_wts)\n",
        "            break\n",
        "\n",
        "    print(\"\\nTraining Complete!\")\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, all_probs, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "9QBIRCLEkikD",
        "outputId": "91f23758-e1e5-4da6-da2c-c68f6ff45595"
      },
      "outputs": [],
      "source": [
        "#torch.save(model.state_dict(), '/content/drive/MyDrive/Model_III_dataset/model_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZJepKUw_txF",
        "outputId": "fb8cea1e-a600-45a2-f604-6052f8bbff56"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\"\"\"\n",
        "Args:\n",
        "        image_size (int): Size of the input image (e.g., 224).\n",
        "        patch_size (int): Size of each patch (e.g., 16).\n",
        "        in_channels (int): Number of input channels (e.g., 1 for your task).\n",
        "        num_classes (int): Number of output classes (e.g., 3 for your task).\n",
        "        embed_dim (int): The main embedding dimension (e.g., 768 for ViT-Base).\n",
        "        depth (int): Number of Transformer Encoder blocks (e.g., 12 for ViT-Base).\n",
        "        num_heads (int): Number of attention heads (e.g., 12 for ViT-Base).\n",
        "        mlp_ratio (float): Ratio to determine MLP hidden dimension (e.g., 4.0).\n",
        "        dropout (float): Dropout probability.\n",
        "\"\"\"\n",
        "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n",
        "batch_size = 32\n",
        "learning_rate = 5e-4\n",
        "weight_decay = 0.05\n",
        "num_epochs = 200\n",
        "warmup_epochs = 10\n",
        "model = VisionTransformer(\n",
        "        image_size=64, patch_size=4, in_channels=1, num_classes=3,\n",
        "                 embed_dim=192, depth=6, num_heads=4, mlp_ratio=4.0, dropout=0.1\n",
        "    )\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "#scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
        "def warmup_lambda(current_epoch):\n",
        "    if current_epoch < warmup_epochs:\n",
        "        return float(current_epoch) / float(max(1, warmup_epochs))\n",
        "    return 1.0\n",
        "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
        "main_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epochs, eta_min=1e-6)\n",
        "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, main_scheduler], milestones=[warmup_epochs])\n",
        "\n",
        "\n",
        "print(\"Optimizer: Adam\")\n",
        "print(f\"Learning Rate: {learning_rate}\")\n",
        "\n",
        "# Train Model\n",
        "model, all_probs, all_labels = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_-lEQtu_txF",
        "outputId": "380b9897-dd4d-40ef-ba22-02663ddea4b9"
      },
      "outputs": [],
      "source": [
        "\"\"\" ROC Curve Plotting Function\"\"\"\n",
        "def plot_roc_curve(all_preds, all_labels):\n",
        "    print(\"Generating ROC Curve\")\n",
        "\n",
        "    # Convert predictions and labels to numpy arrays\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    n_classes = 3\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve((all_labels == i).astype(int), all_preds[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        print(f\"Class {i} ROC AUC: {roc_auc[i]:.4f}\")\n",
        "\n",
        "    # Plot ROC curves\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = ['blue', 'red', 'green']\n",
        "    class_names = ['Axion', 'CDM', 'No Substructure']\n",
        "\n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color,\n",
        "                 label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(figures_dir / \"roc_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(\"ROC Curve saved as roc_curve.png\")\n",
        "\n",
        "\n",
        "plot_roc_curve(all_probs, all_labels)\n",
        "\n",
        "print(\"Training and Evaluation Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ59pFZa_txG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lensid-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
